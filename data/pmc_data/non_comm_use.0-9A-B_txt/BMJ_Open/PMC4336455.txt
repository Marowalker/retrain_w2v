
==== Front
BMJ OpenBMJ OpenbmjopenbmjopenBMJ Open2044-6055BMJ Publishing Group BMA House, Tavistock Square, London, WC1H 9JR bmjopen-2014-00659510.1136/bmjopen-2014-006595Health Services ResearchResearch1506170417301711Bibliometrics of NIHR HTA monographs and their related journal articles Royle Pamela http://orcid.org/0000-0003-0934-4961Waugh Norman Division of Health Sciences, University of Warwick, Coventry, UKCorrespondence to  Dr Pamela Royle; p.l.royle@warwick.ac.uk2015 18 2 2015 5 2 e00659510 9 2014 23 12 2014 30 12 2014 Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions2015This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/Objectives
A bibliometric analysis of the UK National Institute for Health Research (NIHR) Health Technology Assessment (HTA) monographs and their related journal articles by: (1) exploring the differences in citations to the HTA monographs in Google Scholar (GS), Scopus and Web of Science (WoS), and (2) comparing Scopus citations to the monographs with their related journal articles.

Setting
A study of 111 HTA monographs published in 2010 and 2011, and their external journal articles.

Main outcome measures
Citations to the monographs in GS, Scopus and WoS, and to their external journal articles in Scopus.

Results
The number of citations varied among the three databases, with GS having the highest and WoS the lowest; however, the citation-based rankings among the databases were highly correlated. Overall, 56% of monographs had a related publication, with the highest proportion for primary research (76%) and lowest for evidence syntheses (43%). There was a large variation in how the monographs were cited, compared to journal articles, resulting in more frequent problems, with unlinked citations in Scopus and WoS. When comparing differences in the number of citations between monograph publications with their related journal articles from the same project, we found that monographs received more citations than their journal articles for evidence syntheses and methodology projects; by contrast, journal articles related to primary research monographs were more highly cited than their monograph.

Conclusions
The numbers of citations to the HTA monographs differed considerably between the databases, but were highly correlated. When a HTA monograph had a journal article from the same study, there were more citations to the journal article for primary research, but more to the monographs for evidence syntheses. Citations to the related journal articles were more reliably recorded than citations to the HTA monographs.

bibliometricshealth technology assessmentbibliographic databasescitations
==== Body
Strengths and limitations of this study
First study to perform a detailed bibliometric analysis of a cohort of Health Technology Assessment (HTA) monographs and their related outputs published in external journals.

We compared citations to the HTA monographs using three citation databases, and compared citations by type of evidence.

We chose monographs published in 2010 and 2011 in order to allow adequate time for citations to accrue and for the related journal articles to be published.

One limitation is that we obtained details of related publications from the HTA website, where the number of related journal publications may be underestimated by 15.8%.

A possible limitation is that we used the date that a publication was first added to PubMed as a surrogate for the publication date because the actual date of journal publication is difficult to determine.

Introduction
The NIHR HTA monograph series
In the UK, the Health Technology Assessment (HTA) Programme of the National Institute for Health Research (NIHR) is a major funder of research that evaluates health care interventions.1 Set up in 1993, it is the largest and longest running of the NIHR research programmes, and has invested over £647 million over the past 20 years. The Chief Medical Officer for England, Professor Sally Davies, noted in 2013 that there were then 316 active projects, with an expected spend in 2013/2014 of around £75 million.2 The HTA Programme supports policymakers such as the National Institute for Health and Care Excellence (NICE), the National Screening Committee and the Department of Health.

HTA research takes three main forms: primary research (normally trials), evidence synthesis (systematic reviews, which usually include economic modelling) and methodological studies. The research is published in the NIHR HTA Programme's peer reviewed open access journal, Health Technology Assessment (HTA), also known as the HTA monograph series. Each monograph contains the full details of a single study, and as the suggested word limit on the monographs is 50 000, this enables more details of the work to be included compared to a typical peer reviewed journal.3 Authors are also encouraged to publish journal articles from the projects described in the monographs to increase dissemination.4 There may be more than one article from each project; for example, one on clinical effectiveness and one on the economics.

In 2013, the HTA monograph series had a journal impact factor (JIF) of 5.116 and was ranked second of 85 journals in its subject category Health Care Sciences and Services. Research projects are commissioned by the HTA Programme after a prioritisation process to select only the most important. This might be expected to lead to the results of the research being well cited.

The role of bibliometrics in assessing research performance in the UK
Authors of HTA monographs are usually based in academic units, where citations to published research are important as indicators of quality and impact. Citation rates are used for comparing research performance of medical schools or individual departments, assessing individual researchers during appointment or promotions processes (including their h-indices), and judging past performance of researchers applying to grant-awarding bodies for research funds.

RAND Europe has undertaken substantial bibliometric work for the Department of Health in England, and bibliometrics has increasingly come to be used, in combination with peer review, to help inform the Department's funding decisions. For example, the first stage in the NIHR senior investigators award process is a bibliometric assessment by RAND.5

The use of bibliometric data for assessing the performance of universities was considered in a pilot exercise in 2008 and 2009 for the 2013 Research Excellence Framework. The report concluded that “bibliometrics was not sufficiently robust at this stage to be used formulaically or to replace expert review in the REF. However there is considerable scope for citation information to be used to inform expert review”.6 Subsequently, in April 2014, HEFCE announced a new review of metrics, which will build on the pilot exercise and consider the role that metrics-based assessment could play in determining quality, impact and other key characteristics of research undertaken in universities.7

It is known that there is variation in the robustness of bibliometric indictors between disciplines and the comparison of rankings can vary depending on the databases used.6
8 Therefore, given the increasing use of bibliometric indicators in higher education in the UK, it is important that the methods used and the accuracy and coverage of these citation databases are understood.

Citation databases
The three databases currently used for citation analysis are the Web of Science (WoS), Scopus and Google Scholar (GS). The oldest of these is the WoS Core Collection database, produced by Thomson Reuters. It indexes approximately 12 000 of the highest impact journals and conference proceedings, covering all disciplines. The Scopus database was introduced by Elsevier in 2004, and covers nearly 21 000 peer-reviewed journals titles from a wide range of disciplines, as well as books and conference proceedings. GS, also introduced in 2004, is freely accessible, and covers journals and other scholarly literature from a wide range of research areas and formats. Its ‘Cited by’ feature provides access to abstracts of articles that have cited the article being viewed. WoS and Scopus are only available via subscription, and both index the NIHR HTA monograph series. GS is freely available, but unlike the other two databases, it does not provide a list of sources that it indexes. However, it appears to cover all the NIHR HTA monographs, which are also indexed in PubMed.

Aims
Our aim was to undertake a bibliometric analysis of the NIHR HTA monographs and their related journal articles. The steps were to:
Explore the differences in citations to the HTA monographs in GS, Scopus and WoS.

Compare citations to the HTA monographs with journal articles from the same project.



Methods
Downloading the HTA monographs
PubMed was searched to identify all of the articles in 2010 (volume 14) and 2011 (volume 15) published in the HTA monograph series. These were downloaded into Endnote and then exported into Excel. We excluded supplements to the monographs that contained summaries of the Evidence Review Groups reports based on the evidence submission to the NICE as part of the single technology appraisal process. The NIHR Journals Library HTA website9 was used to obtain the research type and details of related journal publications for each HTA monograph.

We used PubMed (as well as identifying monographs from the citation databases and HTA website) in order to obtain the Entrez Date (EDAT) field, which indicates the date the article was first added to PubMed. The EDAT was noted for each HTA monograph and its related publications. The bibliographic record of each related publication was then downloaded from PubMed into Endnote and then to Excel. The number of related publications was noted for each HTA report.

JIFS and subject categories
The JIFs and subject categories of the journals of the related publication were obtained from the ISI Web of Knowledge Journal Citation Reports (JCR) 2012. Subjects were classified as either general (Medicine, General & Internal or Multidisciplinary Sciences) or subject specific (all other subject categories).

Citation data
The citations to all HTA monographs and their related publications were obtained from the WoS Core Collection, Scopus and GS; all citation data were collected within the same week (18 July 2014 to 25 July 2014).
Citations in WoS. The Cited Reference Search facility in WoS was used to determine the total number of citations to each HTA monograph. The number of citation variants, the total citations associated with the excluded variants and the number of citations attached to the Full Record in WoS were recorded for each HTA monograph.

Citations in Scopus. A search for each HTA monograph and its related journal publication was performed in Scopus. The HTA monograph reports often appeared under two different forms of the journal name; that is, Health Technology Assessment or Health Technology Assessment (Winchester England).When this occurred, citations to both forms were combined to ensure they were unique citations. The ‘View Cited by’ and ‘Analyse results’ features were used to obtain detailed data on the citing documents for each article. Data on the Source Titles, Author Names, Affiliation names, Countries and Document types were downloaded into Excel for analysis.

Citations in GS. GS was searched using the title of the monograph article, and the number of citations from the Cited by number beneath each reference was recorded.



Data analysis
Descriptive analyses were performed for all variables. Owing to the skewed distribution of the citation data, non-parametric statistical tests were performed and median and IQR values were reported. The Wilcoxon Mann-Whitney test was used to analyse the difference in citations between two groups, the Kruskal-Wallis rank test was used to analyse the differences in citations between three or more groups and Spearman's correlation coefficient was used to compute the correlation between citations and JIF. All analyses were undertaken in Stata V.12.1 (Stata Corporation, College Station, Texas, USA)

Results
Bibliometric analysis of the HTA monographs
There were 111 monographs in HTA that fulfilled our eligibility criteria: 66 from Volume 14 (2010) and 45 from Volume 15 (2011); 58 (52.3%) were evidence syntheses, 42 (37.8%) primary research and 11 (9.9%) methodology.

Comparison of citations using GS, Scopus and WoS
The results of the comparison of citations to the monographs in GS, Scopus and WoS by research type are shown in table 1.

Table 1 Citations to HTA monographs by database and research type

	Total	Median (IQR)	Range	
Google scholar	
All research types	3459	22 (14–39)	2–206	
 Evidence synthesis	2000	27 (17–42)	4–121	
 Primary	976	16.5 (9–26)	2–83	
 Methodology	483	20 (18–48)	9–206	
Scopus	
All research types	2094	13 (7–24)	2–140	
 Evidence synthesis	1203	16 (10–25)	2–70	
 Primary	608	10 (6–17)	3–62	
 Methodology	283	11 (5–25)	4–140	
Web of science	
All research types	1948	13 (7–22)	2–142	
 Evidence synthesis	1054	14.5 (9–22)	3–57	
 Primary	614	10.5 (6–19)	2–57	
 Methodology	280	14 (4–22)	3–142	
HTA, Health Technology Assessment.

Table 1 shows the number of citations found using GS was 78% (3459/1948) more than WoS and 65% (3459/1948) more than Scopus. Also, Scopus found 7% (2094/1948) more than WoS.

However, we noted an apparent anomaly in that the number of citations for primary research was higher in WoS (608) than Scopus (614). Although this difference was small, we would expect the number of citations from Scopus to be at least equal to those from WoS, as the former covers more journals than the latter. On investigation we found that 12 of the 42 primary research articles had more citations in WoS than Scopus. The largest difference was for citations to Shaw et al,10 with 25 more citations in WoS (34 in WoS and 9 in Scopus). We checked the full Scopus bibliographic records (which list the cited references) of each of these 25 citing articles and observed that in all cases, in contrast to the other cited references, the title of the Shaw article was missing, and there was no hyperlink on the reference. This miscoding meant that these 25 citations were not linked to the Shaw et al monograph.

In response to one of the referees for this paper, we include below table 2 showing some examples of differences in numbers of citations to the five 2010 monographs with the largest numbers of citations. Table 2 shows that the biggest difference is between GS and the other two with sometimes little difference between WoS and Scopus.

Table 2 Comparisons of citations to the most frequently cited HTA monographs published in 2010

Publication	Google scholar	Scopus	Web of science	
Song et al11	206	140	142	
Waugh et al12	121	67	40	
Clar et al13	101	70	52	
Cooper et al14	92	67	54	
Mowatt et al15	86	63	57	
HTA, Health Technology Assessment.

Correlation between citations between the databases
The correlation between the rankings of the monographs on the basis of number of citations in the three databases was high, even though the absolute numbers differed. The Spearman's r for the three comparisons was: GS versus Scopus=0.8993, GS versus WoS=0.8804 and Scopus versus WoS=0.8717; p<0.00001 for all comparisons.

Differences in citations between research types
We analysed the differences in the median number of citations in each database by research type and found in all databases it was highest for evidence syntheses, followed by methodology, and fewest for primary research. These differences in citations between research types within each database were statistically significant for GS and Scopus (p=0.0216 and p=0.0268 respectively), but not for WoS (p=0.1312).

Cited reference variants to the monographs in the WoS
A Basic Search to access articles in the WoS opens a screen within the WoS Core Collection, which has a field called Times Cited, and shows the number of times the article has been cited. However, citations can also be investigated using the more time consuming method of a Cited Reference Search. When using the WoS for citation analysis it was noted that 84 (76%) of the monographs had one or more citations from cited reference variants that were not linked to the Times Cited number on the Full Record page, and were only detected when the Cited Reference Search was used. Table 3, using the example of citations to the HTA monograph by Ashfaq et al,16 shows the variant citations typically seen in the WoS when using a WoS Cited Reference Search.

Table 3 Example of the results for a Web of Science Cited Reference Search for a Health Technology Assessment monograph

Authors	Journal	Year	Volume	Issue
number	Page
numbers	Citing
articles	View record	
Ashfaq, K	HEALTH TECHNOL ASSES	2010	14	54	1	7	View Record in Web of Science Core Collection	
Ashfaq, K	HLTH TECHNOL ASSESS	2010	14	54		1		
Ashfaq, K	HEALTH TECHNOL ASSES	2010	14	54	11	1		
Ashfaq, K	HEALTH TECHNOL ASSESS	2010	14	54	ii	3	View Record in Medline	
Ashfaq, K	HLTH TECHNOL ASSES	2010	14	54	ix	2		
The HTA monograph recommends the report to be cited as: ‘Ashfaq et al16’. However, PubMed requires page numbers when indexing journals (the HTA monographs are indexed as journal articles in the bibliographic databases). The situation is complicated by the monographs having the first pages with Roman numerals followed by the rest in Arabic numbering. So this monograph is referenced in PubMed as: “Health Technol Assess 2010;14:iii–iv, ix–xi, 1–141”.

The first row of data lists the number of citing articles from the WoS Core Collection and records only seven citations that are linked to this record: therefore if a Basic Search is performed, it will appear that this article has only been cited seven times. The second to fifth rows are additional citations found using the Cited Reference Search, and show variants in how the monograph has been cited, with variations in abbreviations of the title (with Health and Hlth, and Asses and Assess), and in page numbering. These rows show that half the citations are missed if only a Basic Search is used, because these citations are not linked to the record in the Core Collection.

Overall, 30 monographs had citations to one unlinked cited reference variant, and the remaining 54 records had between 2 and 12 variants with unlinked citations. This resulted in a total of 25.5% (491) citations to the monographs being unlinked to their Full Record in the WoS Core Collection.

As Scopus does not have an equivalent Cited Reference Search function it was not possible to determine the unlinked citations due to cited reference variants in this database. The remaining citation analyses were performed using Scopus, unless otherwise stated.

Further results from citation analysis in Scopus
An analysis of citations to the HTA monographs showed the following:
The authors citing the monographs came from the UK (24.1%), followed by the USA (17.5%), Germany (5.7%), Canada (4.8%) and Australia (4.4%). The remaining 43.5% were from 75 other countries.

The journal that most commonly cited the HTA monographs was PLOS ONE (6.8%), followed by the citations for the HTA journal (3.5%). The remaining citations came from 78 different journals.

The document types that cited the HTA monographs were: 63.4% from articles, 26.5% from reviews, 2.9% from editorials, with 7.2% from other document types.



Bibliometric analysis of journal publications related to the HTA monographs
Number of related publications
Overall, 56% (62) of the monographs had published one or more related journal publication; of these monographs, half (31) had one journal publication, and half had between 2 and 8-related publications. Of the monographs that had at least one related journal article, the proportions for each research type were: 76% primary research, 43% evidence synthesis and 56% methodology. There was no significant difference between citations to the monographs that had a related journal publication and those that did not (p=0.942).

Cited reference variants to the journals in WoS
Fewer of the related journal articles had cited reference variants unlinked to the source items in WoS than was seen for the monographs (see above): that is, 29% of the journal articles had at least one unlinked citation (only viewable in a Cited Reference Search) and the number of variants per article ranged from 1 to 5, also fewer than for the monographs.

Citations to the related journal articles by research type
The number of citations to the related journal articles, broken down by research type, are shown in table 4. Of the 128 journal publications in Scopus, 45 (35%) were from evidence synthesis monographs, 69 (54%) from primary research and 14 (11%) from methodology.

Table 4 Scopus citations to the related journal publications

	Total citations	Medians	IQR	Range	
All research types (n=128)	4850	11	4–25	0–707	
 Evidence Synthesis (n=45)	604	6	4–22	0–73	
 Primary (n=69)	3358	12	4–27	0–707	
 Methodology (n=14)	888	12	4–82	1–308	
Although primary research and methodology journal articles had a higher median number of citations (12 for each) than evidence synthesis (6), there was no statistically significant difference (p=0.3516) in citations between the research types. However, the ranges of citations were much wider for primary research and methodology (707 and 307 respectively) than evidence syntheses (73).

Citations and JIFs
The related publications were most frequently published in The BMJ (16 articles) and the Lancet (6). The remaining 112 articles were published in 79 different journals. For the 127 articles that were in a journal with a JIF, the mean JIF was 7.202, and the range was from 0.768 to 51.658. The mean JIFs for each research type were: evidence synthesis=4.698, primary research=9.957 and methodology=5.040. There was a moderate correlation between the citations to the related journal publications and their JIF; Spearman's r=0.5561 (p<0.00001).

Countries of authors citing the journal publications
Of the citing authors, 21% came from the UK, 20% from the USA, followed by Germany (6%), Canada (5%) and Australia (5%) The remaining citations were from 91 other countries.

Difference in the citations between the monograph and journal versions
In most cases, monographs were added to PubMed at about the same time (a median of 0.03 months difference, IQR −10.71 to +0.33 months), but the range was wide, that is, with one journal article appearing 9 years before its monograph and one monograph appearing 4 years before the journal article. Such large differences in publication dates mean that some monographs and articles will vary substantially in the time they have had to accrue citations. Therefore, we compared numbers of citations to monographs with their related journal articles in a subset containing only those 80 pairs of monograph and journal article that were published within 12 months of each other.

Overall, there were slightly fewer (3) citations to the related journal articles than to their monographs, but these differences in citations varied significantly by research types (p=0.0001). For evidence syntheses and methodology the monograph was more frequently cited (by a median of 6 and 29 citations, respectively) than its related journal article; this is in contrast to primary research, where the journal articles were more frequently cited than their monographs by a median of four citations. The differences in citations between the monograph and its related journal articles were highly skewed, but mostly driven by some highly cited primary research articles published in general medical journals, with the citations to the journal article greatly exceeding those to their related monographs. The largest difference was to the Lancet article by Peek et al,17 which received 544 citations, compared to its monograph,18 which received 32 citations. The top 10 percent (8 articles) of the differences favouring citations to the journal articles all came from primary research published in general medical journals (The BMJ=2, Lancet=4, New England Journal of Medicine=1, JAMA=1).

Discussion
This comparison of three citation databases showed that overall GS gave 78% more citations than WoS and 65% more than WoS, and Scopus gave 7% more than WoS; however, the rankings of numbers of citations among the databases were highly correlated. When citations to the 111 HTA monographs were analysed by research type, all databases showed the highest number of citations to evidence syntheses, then methodology, and fewest for primary research. The lower proportion of citations for primary research monographs may be because there were more journal articles published from the primary research projects, with at least one journal article from 76% of primary research, compared to 56% from methodology and 43% from evidence syntheses.

When overall citations to the related journal articles were analysed by research type, primary research received the highest number of citations and evidence syntheses the fewest. A comparison of differences in the number of citations in Scopus between monograph publications paired with each of their related journal articles from the same project, found that the monographs received more citations than their related journal article(s) for evidence syntheses and methodology projects, but for primary research the journal articles were more highly cited than their monograph version. The monographs and journals were both shown to have a large international readership, as 76% and 79%, respectively, of the citing authors came from outside the UK, with the largest percentage of these coming from the USA. Also, the journal that most cited the monographs, PLOS ONE, is based in the USA.

The strengths of this study were that it was the first study to perform a detailed bibliometric analysis of a cohort of HTA monographs and their related outputs published in external journals. We compared citations to the HTA monographs using three citation databases, and compared citations by type of evidence. A limitation is that we obtained details of the related journal publications from the HTA website,9 where the number of publications may be underestimated by 15.8%.19 However, this should not affect our results unless there is some systematic bias in reporting of external publications.

Also, we used the date that a publication was first added to PubMed as a surrogate for the publication date; this was because the actual dates of journal publications are difficult to determine, as some journals are electronic only and many print journals now make their articles available online before they appear in print20 (these epub ahead of print articles are included in PubMed). However, there may be variation among journals with respect to the time taken to be included in PubMed.

Although the absolute number of citations to the individual monographs usually differed significantly between GS, Scopus and WoS (eg, the Waugh et al12 monograph received 121 citations in GS, but 67 in Scopus and 40 in WoS), the rankings of the monographs based on the citations were highly correlated between the three databases. In the recent responses to the call for evidence for the review of the role of metrics in the assessment of research,7 it was reported that 17 universities were using Scopus, 15 were using GS and 18 were using WoS. Hence it is important that individuals quoting citation numbers for their papers (eg, on applications for jobs, promotion or grants) specify the database they used, to ensure that there is a fair comparison between researchers from different institutions.

Higher numbers of citations using GS compared to Scopus and WoS was also found by Kulkarni et al.21 This is understandable, due to the greater range of types of literature GS covers, such as non-peer reviewed and grey literature. The high correlation between the ranking of citations using GS and WoS, as observed in our study, has also been observed by Kousha et al.22 Also, in a comparison of the number of citations using WoS and GS for articles by 20 Nobel Prize Winners in four disciplines, Harzing et al8 found their ranking for Medicine to be nearly identical for GS and WoS data, but for the other disciplines the rankings differed between the two databases.

Our data showed that 56% of the monographs had published at least one external journal article, but the proportions by research type varied; that is, 76% primary research, 56% methodology and 43% evidence syntheses. The higher proportion of HTA primary research being published in an external journal was also found by Chinnery et al,19 who reported that 62% of primary research and 44% of evidence synthesis monographs were also published in an external journal. This higher proportion of journal articles coming from primary research may be because journals consider primary research more newsworthy than reviews of already published evidence. In our study, the monographs of evidence syntheses and methodology were more highly cited than their related journal articles, whereas journal articles of primary research (especially those in general medical journals) were more highly cited than the monographs. Primary research also appeared in journals with higher impact factors.

The monographs have the advantage over their related journal articles in that they allow for more complete description of the study. However, we found that monograph authors could be potentially disadvantaged in a bibliometric analysis if those using the WoS Core Collection do not perform a Cited Reference Search to capture all citations. This was because 76% of the monographs in the WoS had at least one cited reference variant unlinked to their full record (this was 2.6 times more frequent than for the related journal articles), and this resulted in 26% of monograph citations being missed unless the much more time consuming method of a Cited Reference Search was used. The major source of unlinked citations to the monographs in WoS appeared to be due to variations in how the journal name, issue and page numbers (the monographs are paginated using Roman as well as Arabic numerals) were cited.

We also noted inaccuracies in citations to monographs in Scopus. Our detailed analysis of one monograph for the reasons for the missing citations (ie, those that were in WoS but not Scopus) showed incomplete entries of the cited monographs in Scopus and missing citation links to the monograph. Such errors were not noted for the other articles cited alongside the monographs. We speculated that the number of variations in the form in which HTA journal is cited contributed to the much higher rate of unlinked citations to the monographs in Scopus.

Publishing in an external journal can improve dissemination of findings. Clinicians are likely to prefer a short journal article compared to a 170-page monograph. However, academics who publish journal papers from their monograph may spread their citations across more papers and hence reduce their h-index; that is, an academic with an h-index of 30 will increase that by publishing one paper that receives 32 citations, but not by publishing two papers that are cited 16 times each. The h-index is, of course, only one of a number of citation indicators, and has advantages and disadvantages (summarised in the RAND report5), but the h-index is easily available, and we know it is being used in medical schools.

The decision on publication strategy will depend on several factors: the key audience, dissemination and citations. Publishing primary research in general medical journals appears to be a good strategy for maximising citations. The HTA monograph series is entirely open access, which might be thought to increase numbers of citations. However, Davis23 reported that in a randomised trial, open access articles were cited no more often than subscription-access ones.

Problems with indexing in WoS have been reported by a Canadian study exploring the use of bibliometric indicators for rating researchers.24 The Universities UK report also noted that not all citations are collated, partly because authors use a variety of abbreviations for journal titles.25 Meanwhile, people using citation databases for bibliometric assessments should know about possible sources of error in citation databases, and how best to use them, or should seek advice from experts.

Future research could investigate if the relative citations and rankings between the different citations databases found in this study are generalisable to other disciplines, and which citation database gives the most reliable and consistent results. Also, it has been reported that a higher proportion of statistically significant findings are reported in journal articles when compared with the outcomes reported in HTA monographs.26 Therefore, it would be interesting to see whether monographs and journal publications with significant findings are cited more often than those with non-significant outcomes.

Conclusions
The numbers of citations to the HTA monographs differed considerably between the databases, but rankings were highly correlated. When a HTA monograph had a journal article published from the same study, there were more citations to the related journal article for primary research, but more to the monographs than their related journal articles for evidence syntheses. Citations to related journal articles were more reliably recorded than citations to the HTA monograph series, which showed many variant citation forms, leading to unlinked citations. Those using bibliometric data for decisions regarding academic career progression and research funding need to be aware of how differences among databases will affect their results. Publishing in an external journal as well as a monograph can aid dissemination of the work and increase overall citations to the work; alternatively, if the related journal article is cited instead of the monograph, it could reduce the citations to the monograph. However, there does seem to be a clear citation advantage in publishing primary research articles in general medical journals, as well as via the monograph series.

Contributors: PR and NW conceived the study; PR designed the study, extracted the data and carried out all analyses; PR and NW drafted the manuscript and approved the final version.

Funding: Funding received from University of Warwick.

Competing interests: The authors publish reports in the Health Technology Assessment monograph series and sometimes as related journal articles.

Provenance and peer review: Not commissioned; externally peer reviewed.

Data sharing statement: No additional data are available.
==== Refs
References
1 Wright D , Young A , Iserman E  
The clinical relevance and newsworthiness of NIHR HTA-funded research: a cohort study . BMJ Open 2014 ;4 :e004556 
doi:10.1136/bmjopen-2013-004556
2 Davies S  
NIHR and the HTA Programme. Secondary NIHR and the HTA Programme 2013. http://www.profbriefings.co.uk/nihrhta2013/presentations/Plenary%20Day%201%20Sally%20Davies.pdf (accessed 21 Dec 2014 ).
3 Douet L , Milne R , Anstee S  
The completeness of intervention descriptions in published National Institute of Health Research HTA-funded trials: a cross-sectional study . BMJ Open 2014 ;4 :e003713 
doi:10.1136/bmjopen-2013-003713
4 Turner S , Wright D , Maeso R  
Publication rate for funded studies from a major UK health research funder: a cohort study . BMJ Open 2013 ;3 :e002521 
doi:10.1136/bmjopen-2012-002521
5 Ismail S , Nason E , Marjanovic S  
Bibliometrics as a tool for supporting prospective R&D decision-making in the health sciences: strengths, weaknesses and options for future development: RAND Technical Report Secondary Bibliometrics as a tool for supporting prospective R&D decision-making in the health sciences: strengths, weaknesses and options for future development: RAND Technical Report 2009. http://www.rand.org/pubs/technical_reports/TR685.html (accessed 10 Sep 2014 ).
6 Higher Education Funding Council for England (HEFCE) . Report on the pilot exercise to develop bibliometric indicators for the Research Excellence Framework. Secondary Report on the pilot exercise to develop bibliometric indicators for the Research Excellence Framework, 2009. http://www.hefce.ac.uk/pubs/year/2009/200939/ (accessed 10 Sep 2014 ).
7 Higher Education Funding Council for England (HEFCE) . Independent review of the role of metrics in research assessment. Secondary Independent review of the role of metrics in research assessment 2014. http://www.hefce.ac.uk/whatwedo/rsrch/howfundr/metrics/ (accessed 10 Sep 2014 ).
8 Harzing AW  
A preliminary test of Google Scholar as a source for citation data: a longitudinal study of Nobel prize winners . Scientometrics 2013 ;94 :1057 –75 
doi:10.1007/s11192-012-0777-7
9 National Institute for Health Research . NIHR Journals Library: Health Technology Assessment. Secondary NIHR Journals Library: Health Technology Assessment 2014. http://www.journalslibrary.nihr.ac.uk/hta (accessed 10 Sep 2014 ).
10 Shaw L , Rodgers H , Price C  
BoTULS: a multicentre randomised controlled trial to evaluate the clinical effectiveness and cost-effectiveness of treating upper limb spasticity due to stroke with botulinum toxin type A . Health Technol Assess 2010 ;14 :1 –113 , iii–iv 
doi:10.3310/hta1426020515600 
11 Song F , Parekh S , Hooper L  
Dissemination and publication of research findings: an updated review of related biases . Health Technol Assess 2010 ;14 :iii , ix–xi, 1–193 
doi:10.3310/hta1408020181324 
12 Waugh N , Cummins E , Royle P  
Newer agents for blood glucose control in type 2 diabetes: systematic review and economic evaluation . Health Technol Assess 2010 ;14 :1 –248 
doi:10.3310/hta14360
13 Clar C, Barnard K, Cummins E, et al. Self-monitoring of blood glucose in type 2 diabetes: systematic review. Health Technol Assess 2010;14:1–140.
14 Cooper K , Squires H , Carroll C  
Chemoprevention of colorectal cancer: systematic review and economic evaluation . Health Technol Assess 2010 ;14 :1 –206 
doi:10.3310/hta14320
15 Mowatt G , Zhu S , Kilonzo M  
Systematic review of the clinical effectiveness and cost-effectiveness of photodynamic diagnosis and urine biomarkers (FISH, ImmunoCyt, NMP22) and cytology for the detection and follow-up of bladder cancer . Health Technol Assess 2010 ;14 :1 –331 , iii–iv 
doi:10.3310/hta14040
16 Ashfaq K , Yahaya I , Hyde C  
Clinical effectiveness and cost-effectiveness of stem cell transplantation in the management of acute leukaemia: a systematic review . Health Technol Assess 2010 ;14 :1 –141 
doi:10.3310/hta14540
17 Peek GJ , Mugford M , Tiruvoipati R  
Efficacy and economic assessment of conventional ventilatory support versus extracorporeal membrane oxygenation for severe adult respiratory failure (CESAR): a multicentre randomised controlled trial . Lancet 2009 ;374 :1351 –63 
doi:10.1016/S0140-6736(09)61069-219762075 
18 Peek GJ , Elbourne D , Mugford M  
Randomised controlled trial and parallel economic evaluation of conventional ventilatory support versus extracorporeal membrane oxygenation for severe adult respiratory failure (CESAR) . Health Technol Assess 2010 ;14 :1 –46 
doi:10.3310/hta1435020642916 
19 Chinnery F , Young A , Goodman J  
Time to publication for NIHR HTA programme-funded research: a cohort study . BMJ Open 2013 ;3 :e004121 
doi:10.1136/bmjopen-2013-004121
20 Tort AB , Targino ZH , Amaral OB  
Rising publication delays inflate journal impact factors . PLoS ONE 2012 ;7 :e53374 
doi:10.1371/journal.pone.005337423300920 
21 Kulkarni AV , Aziz B , Shams I  
Comparisons of citations in web of science, Scopus, and Google Scholar for articles published in general medical journals . JAMA 2009 ;302 :1092 –6 
doi:10.1001/jama.2009.130719738094 
22 Kousha K , Thelwall M  
Google scholar citations and google Web/URL citations: A multi-discipline exploratory analysis . J Am Soc Inf Sci Technol 2007 ;58 :1055 –65 .
23 Davis PM  
Open access, readership, citations: a randomized controlled trial of scientific journal publishing . FASEB J 2011 ;25 :2129 –34 
doi:10.1096/fj.11-18398821450907 
24 Campbell D , Picard-Aitken M , Côté G  
Bibliometrics as a performance measurement tool for research evaluation: the case of research funded by the national cancer institute of Canada . Am J Eval 2010 ;31 :66 –83 
doi:10.1177/1098214009354774
25 Universities UK . The use of bibliometrics to measure research quality in UK higher education institutions. Secondary The use of bibliometrics to measure research quality in UK higher education institutions 15/12/2014 2007. http://www.universitiesuk.ac.uk/highereducation/Documents/2007/Bibliometrics.pdf (accessed 29 Sep 2014 ).
26 Matthews GA , Dumville JC , Hewitt CE  
Retrospective cohort study highlighted outcome reporting bias in UK publicly funded trials . J Clin Epidemiol 2011 ;64 :1317 –24 
doi:10.1016/j.jclinepi.2011.03.01321889307

