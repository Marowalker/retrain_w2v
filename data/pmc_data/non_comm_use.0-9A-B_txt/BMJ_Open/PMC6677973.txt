
==== Front
BMJ OpenBMJ OpenbmjopenbmjopenBMJ Open2044-6055BMJ Publishing Group BMA House, Tavistock Square, London, WC1H 9JR bmjopen-2019-03101410.1136/bmjopen-2019-031014Medical Education and TrainingResearch15061709Educational impact of assessment on medical students’ learning at Tehran University of Medical Sciences: a qualitative study Kordestani Moghaddam Azadeh 1Khankeh Hamid Reza 23Shariati Mohammad 4Norcini John 5Jalili Mohammad 16
1 
Department of Medical Education, Tehran University of Medical Sciences, Tehran, Iran

2 
Health in Emergency and Disaster Research Center, University of Social Welfare and Rehabilitation Sciences, Tehran, Iran

3 
Department of Clinical Science and Education, Karolinska Institute, Stockholm, Sweden

4 
Department of Medical Education, Department of Community Medicine, School of Medicine, Tehran University of Medical Sciences, Tehran, Iran

5 
Foundation for Advancement of International Medical Education and Research, Philadelphia, Pennsylvania, USA

6 
Department of Emergency Medicine, School of Medicine, Health Professions Education Research Center, Tehran University of Medical Sciences, Tehran, Iran
Correspondence to  Dr Mohammad Jalili; mjalili@tums.ac.ir2019 29 7 2019 9 7 e03101414 4 2019 26 6 2019 04 7 2019 © Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.2019This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.Objectives
It has been shown that assessment strongly affects students’ performance. A deeper insight needs to be gained into the interplay of assessment and learning. The aim of the current study was to develop a model to explain the educational impact of assessments on students’ learning, before, during and after the test.

Design
This study used semistructured interviews, focus group discussions and observation and collection of field notes. A qualitative methodology using the grounded theory data analysis approach was then used to generate an explanation of the process of how assessment impacts students’ learning.

Setting
School of Medicine, Tehran University of Medical Sciences.

Participants
Participants were medical students and teachers with first-hand experience or expertise in assessment as well as their willingness to participate in the study. Fifteen people (eight medical students, seven faculty members) were interviewed. One focus group discussion (with five students) was held.

Results
The extracted concepts from our study were classified into four main categories. These categories include elements of the assessment programme which affect learning, the mechanism through which they exert their effects, contextual factors and the impact they have on learning. These elements and their interplay occur within an environment with its antecedent characteristics.

Conclusions
This study suggested a model for understanding the elements of the assessment which, within the context, affect learning, the mechanisms through which they impart their effects and the final outcomes obtained.

educational impactassessmentstudents’ learningqualitative studyhttp://dx.doi.org/10.13039/501100004484Tehran University of Medical Sciences and Health Services9121486007.2015-3-11special-featureunlocked
==== Body
Strengths and limitations of this study
This study developed a comprehensive, yet easy to understand, model to illustrate the educational consequences of assessment by categorising the findings into the four components of contributing factors, mechanisms of action, educational consequences and contextual factors.

We tried to explore the attitudes and experiences of diverse groups of stakeholders, students and faculty members, and multisources of data gathering were used.

Voluntary participation in the study and the setting of a single institution may restrict the transferability of our results to other contexts.

Although the study aimed at both formative and summative assessments, since the participants’ experiences in formative assessments were limited, the results of the study seem to focus more on summative assessment.

We studied undergraduate students in this study. For further validation and generalisation of this model, it is suggested that future studies be conducted on how assessment affects postgraduate students’ learning.

Introduction
‘Assessment drives learning’. This short and well-known quote explicitly states that there is a relationship between assessment and learning.1 Assessment can influence the amount and the quality of the study, as well as the allocation of student’s efforts.2 Although there might be arguments regarding when and how the effects of assessment are exerted, the existence of such a relationship has never been questioned. The impact of examinations on students' learning, often referred to as the ‘educational impact’ of assessment,3 the ‘testing effect’, ‘consequential validity’, ‘test-enhanced learning’, ‘backwash’, ‘washback’ and ‘testing phenomenon’,4 is an important element of the utility of an assessment system.5 The testing effect on learning, while meant to be positive, is not invariably so. For example, obtaining a bad score in a test may discourage a student from further studying.4


Although examinations have always been an integral part of the educational process,6 they have traditionally been used solely to assess students' knowledge and skills. This summative purpose of assessment was often justified by the social accountability of the schools to ensure that the graduates are competent. At best, a driving role was also considered for examinations, stating that without them, students will not have enough incentive for studying, a claim that has also been confirmed in some empirical studies.7–9 In recent decades, however, many experts believe that the first and most important goal of assessment should be to maximise students’ competence and to guide them for subsequent learning.6 10 In fact, the paradigm shift from ‘assessment of learning’ to ‘assessment for learning’ is gaining more and more popularity.6 Several studies have suggested that examination affects students’ performance more strongly than the educational methods; the study by Raupach et al is one example.7 This paradigm shift emphasises the need for a deeper insight into the interplay of assessment and learning. One will not be able to design a proper assessment system that actually enhances learning without having a deep insight into these interactions.4


An exploratory study of the existing literature revealed several studies and review papers11 12 dealing with the effect of assessment on learning. While a number of studies have specifically examined the effect of formative assessment and feedback on learning,13–19 some others have evaluated the role of summative assessments on learning.4 9 20 Many studies have looked into the relationship between general assessment factors and students’ approaches to learning.11 21–28 Some have focused on the impact of specific types of examination such as clinical assessments or portfolio on students’ learning.17 19 22 29 Broekkamp et al presented a theoretical model that integrates various factors related with test readiness and learning strategies. Factors mentioned in this paper included teacher’s intended task demands, learners’ perceptions of test demands, learners’ goals for studying and their ability to apply the strategies.11 Al Kadri et al explored how assessment affects surface and deep learning, the impact of post-test feedback on student learning and the effects of assessment characteristic on students’ approaches to learning,24 29 but do not provide a holistic view of the educational impact of assessment in terms of when and how the effects are exerted and what the final outcomes are. Although Dochy and colleagues organised their findings on the impact of assessment on learning under the titles of before, during and after the test,2 the only original study that tried to examine the impact of assessment on student learning and propose a representative model is the study by Cilliers et al.9 This study, while very informative and inspiring, was focused on pre-assessment learning effects of summative assessments. In this study, the impact of assessment on students’ learning during and after assessment has not been studied. Moreover, this model needs to be carried out in different societies and contexts for further validation and generalisation of the findings.6 To the best of our knowledge, there is no comprehensive study that assesses the impact of assessment (either formative or summative) on students’ learning before, during and after examination. The purpose of the current study was to develop a model to illustrate the educational impact of assessments on students’ learning before, during and after the examination.

Methods
Setting
Doctor of Medicine programme at Tehran University of Medical Sciences is a 7-year programme consisting of four phases of basic sciences, foundations of clinical medicine, clerkship and internship. During the first 2.5 years, when basic sciences courses are taught, assessment is mainly performed by summative end-of-term written examinations, followed by a national ‘comprehensive basic sciences examination’ at the end of the phase. In foundations of clinical medicine, these written tests are often augmented by some clinical examinations including an objective structured clinical examination (OSCE), which mainly focuses on physical examination and history taking skills. During the next phases, most evaluations are conducted clinically, often through global evaluation by attending physicians and occasionally through oral examinations. Some major clerkship rotations take formal end-of-rotation written examinations. Assessment during internship is mainly clinical and by end-of-rotation global rating performed by faculty members or senior residents.

While trainees are constantly being supervised by faculty members and senior residents during their everyday practice and learning activities in clinical rotations and informal feedback is frequently provided, formal formative assessment is rarely undertaken and our educational system tends to focus primarily on summative use of the assessment results.

Study design
This study employed a qualitative methodology using constructivist grounded theory approach of Corbin and Strauss.30 This is a suitable approach to discover new areas of the subject, or to explore a known area from a new perspective.30 Another reason for using the grounded theory approach is the discovery of processes and social interactions.31 Since the educational impact of assessment system is an interactive process, and this process is a product of social interactions, and some aspects of it were not well understood, we considered using the grounded theory approach a suitable study method.

The study protocol was granted approval.

Participants and sampling
An iterative purposive sampling technique30 was used to recruit participants from a single, large medical school. Participants were selected according to their first-hand experience or expertise in assessment as well as their willingness to participate in the study. We followed open sampling method with maximum diversity for selection of the participants. In order to have maximum diversity, medical students from different phases of the curriculum (basic sciences, foundations of clinical medicine, clerkship, and internship) were invited to participate. Furthermore, clinical and basic science teachers with different degrees (assistant professor, associate professor, and professor) were invited (table 1). All participants were presented with the information sheets introducing the researchers and stating the aim of the study; informed oral consent was then obtained. We continued sampling and collecting data until new conditions, attributes or consequences of existing categories were not developed by collecting more data (theoretical saturation).30 32 33


Table 1 Characteristics of participants

Participants	Trainees	Teachers	
Basic sciences	FCM	Clerks	Interm	Basic sciences	Clinical Sciences	
Assistant professor	Associate professor	Professor	Assistant professor	Associate professor	Professor	
Female		2	3		1	1		2			
Male	1	1	4	2					2	1	
Total	1	3	7	2	1	1	–	2	2	1	
FCM, Foundations of Clinical Medicine.

Research team
The research team consisted of a PhD student in medical education and two supervisors, one supervisor who had experience in qualitative study designs and one who had experience and expertise in student assessment.

Data collection and analysis
We applied a within-method triangulation approach for data collection and analysis in order to confirm findings, to ensure collection of comprehensive data and to increase the validity of findings.34 35 For this reason, we used three different methods for data collection. These included individual, semistructured interviews with 15 people (eight medical students, seven faculty members), one focus group discussion with five students, as well as observation and collection of field notes.

The interviews lasted from 40 to 75 min. Primarily, the interview and focus group questions were framed as shown in table 2; further questions were asked based on the participant's response and the emerging theory. All the data from the interviews were recorded and transcribed verbatim by the principal researcher (AKM) without identifying data. In line with the grounded theory data analysis approach, data collection and analysis was conducted in an iterative fashion with data analysis completed prior to the next interview.

Table 2 Primary interview and focus group questions

Target Population	Questions	
Teachers	What methods of assessment do you often use in different courses?	
What is, in your opinion, the relationship between assessment and learning?	
What factors determine which method you use to assess students?	
How do you provide feedback to students after assessments?	
Trainees (interview)	What kinds of assessment have you experienced and how did you prepare for them?	
Do you think assessments have anything to do with your learning? How would your learning change if you did not have to sit a test?	
Explain which factors related to the assessment will affect your learning and in what way?	
Could the assessment affect your learning during the testing session or after that? How?	
Trainees
(focus group)	What kinds of assessment have you experienced and how did you prepare for them?	
Do you think assessments have anything to do with your learning? How would your learning change if you did not have to sit a test?	
Explain which factors related to the assessment will affect your learning and in what way?	
Could the assessment affect your learning during the testing session or after that? How?	
How are you given feedback on your performance after examination? How does that affect you?	
After the eighth interview with the students, the focus group discussion meeting was held to clarify the primary results and find new ideas. Attempts were made to facilitate discussions and involve all parts of assessment process. In the focus group discussion, we observed the interactions between the group members and exchanging views among the participants about their experiences. In this way, we noted different points of view and the reaction of the members of the group to others. This helped us to better understand the dimensions of the study process.36 37 The data generated were used as complementary data and for trustworthiness as well. A focus group meeting with teachers had been tentatively planned but deemed unnecessary later.

One of the researchers (AKM) observed two OSCEs and took field note and memos in an attempt to appreciate the interaction between trainees and examination as well as the test structure and the verbal and non-verbal interactions between students and assessors. Moreover, field notes were used to record the students' non-verbal behaviours in individual interviews and focus group discussion. The field notes enhanced reflection in relation to the assessment process in the researcher and led to the recording of memo. Memos were probed in subsequent interviews.38


The verbatim transcriptions of all the interviews and focus groups and field notes were then integrated.

A constant comparative approach was used to analyse data, simultaneous with data collection.30 Open coding (line by line reading of the transcripts and field notes, and assigning the relevant code to it), axial coding (categorisation of cods, and determining the relationship between concepts) and selective coding (integration of categories and process description) methods were employed to reveal the processes involved. An extracted example of the coding and analysis process is shown in table 3. Transcripts were read in detail several times and emerging themes were identified. They were repeatedly reexamined and ongoing comparisons were made, as further interviews were performed. This method leads to the development of a robust coding scheme for thematic organisation and classification of data. After refinement of the coding scheme, the data were analysed in order to reach a conceptual level. The relationships among the concepts were extracted and an interpretive model was identified. This model was supposed to explain and justify the data and render them meaningful.

Table 3 An example of the process of abstraction, from meaning unit to category

Meaning unit	Code	Subcategory	Category	
The teacher himself has to accept that the test can help students learning and is not merely a means of scoring. Teachers do not agree that the type of test questions affects learning.	Inappropriate teacher's attitude toward assessment	Attitude toward assessment	Assessor-related factors	
Typically, teachers who are professors think they should not devote their time to designing a question for a student’s assessment.	Lack of motivation due to lack of performance evaluation	Motivation for assessment	Assessor-related factors	
Designing a good question that we can measure through different levels of learning is very difficult, and teachers need to be educated in this regard.	Lack of skill to design good questions	Capability of assessment	Assessor-related factors	
Young teachers are usually stricter. Somehow they are fresh and point to the point of the book, they think we should also point to the book	The impact of teachers experience on the stringency of assessment	Leniency/
stringency	Assessor-related factors	
Patient and public involvement
No patient involved.

Trustworthiness
In this study, trustworthiness was performed based on proposed criteria of Lincoln and Guba.39 The triangulation strategy for data gathering (semistructured interviews, focus group, observation, field notes) and prolonged engagement with participants during data collection process were used to ensure the credibility.40 In order to achieve the dependability and confirmability,39 a summary of analysed interviews and extracted codes was sent for the participants (member check). Also, data were shared with two PhD students who had a qualitative research experience and were not in the research team. Moreover, the research team met several times to review and discuss the emerging data, and the results of the study were discussed in an expert panel (external check). In order to ensure transferability, we provided a thorough description of the context and characteristics of the participants, describing the context of the study, and a clear description of the barriers and limitations and conditions for using the findings.40


Results
The experiences of students and faculty members were investigated regarding the effect of assessment on students' learning through interviews, focus group meetings and observations. The results of the analyses of their experiences are the findings of this study.

The extracted concepts from our data are presented as a model in figure 1. We classified our findings into four main categories: the elements of the assessment programme which affect learning (contributing factors), the mechanisms through which they exert their effects (mechanism of action) and the impact they have on learning (educational consequences). These elements and their interplay occur within an environment with its antecedent characteristics (contextual factors).

Figure 1 The suggested model for the educational impact of assessment.

Contributing factors
The elements of the assessment system that are proposed in this study as being effective in its educational consequence can be classified into three categories, including those related to the assessor, elements related to the assessee and elements related to the assessment process.

Assessor-related factors
Assessor is one of the influential elements of the assessment system. Assessor’s attitude towards the assessment, their incentive for assessment, their capabilities to do the assessment as well as their leniency/stringency can influence the learning which results from student assessment. The assessor’s attitude toward how to facilitate learning through assessment and the relationship between learning and assessment objectives is important. Moreover, feedback from the institute and students which are received by the assessor regarding previous assessments may influence the assessment process. The extent to which training and assessment are among the professional priorities of the assessor is another category that can influence the incentives of the assessor in the assessment process. On the other hand, the assessor’s knowledge of the assessment objectives and having sufficient skills for design and implementation of different assessment methods help the assessor to have an appropriate assessment. Whether the assessor expects the students to memorise the details of the content or focuses mainly on understanding the important concepts is another factor that influences students’ learning. All of these factors, except leniency/stringency, indirectly affect the students’ learning through the impact on the design of the test. The leniency/stringency of assessor can affect the design of the test as well as test scoring. In addition, the performance of the assessor in providing information about the objectives of assessment, how to perform assessment and how to guide students about the questions can influence students’ learning.

Quotation 1: If you expect students to behave in a specific way, you should teach them first and then assess them. When you do not do it yourself and you do not included it in the teaching, then it is not fair to put it in your exam (T1).

Quotation 2: In my opinion, we need training to be able to make good questions. It is difficult to design an appropriate case-based question to assess different levels of learning (T2).

Assessee-related factors
Personal characteristics of the assessee, their motivation and their perception of the examination all influence their learning from assessment. Personal characteristics of the assessee such as the level of stress experienced by the student before the examination, as well as their ambitiousness and competitiveness are among the factors that can impact students’ learning. Students’ incentives for study may vary from successful performance in the examination to learning the content and achieving proficiency. Students’ understanding of the nature of examination, including awareness of the goal, content and format of the examination, as well as their perception regarding the examination, which may stem from the information provided by the system or assessor or from their experiences in previous exams, determine the type of study that is necessary to become successful.

Quotation 3: The behaviour of the teacher and how he acted in previous semesters is very important. For examples, we knew that that Mr. X takes an exam but does not actually check the papers; so, we simply ignored the exam (S2).

Quotation 4: We were not like this from the very beginning, but according to the scores that we obtained, we concluded that we should assess the exam first and then plan how to study according to the exam type (S6).

Factors related to method
Several factors associated with the assessment method may contribute to its educational consequence. These include assessment format, stakes of the examination, assessment content, as well as scoring and standard setting process, examination duration, frequency and the interval between the examinations. Use of practical and clinical tests besides written knowledge tests encourages students to obtain the necessary skills. Also, the share of each examination in final assessment influences students’ amount of study and learning. One of the important factors that influence the amount and type of study is the stakes of the test. Since success in high stake examinations is more important for students, they experience more stress compared with other examinations and allocate more time preparing for them. Students’ experiences have shown that the stress experienced by them before the examination can influence the speed of study and the amount of learning. Furthermore, students stated that sufficient time before examinations can encourage them to study. Furthermore, the experiences of students and faculty members showed that frequent assessments with short intervals in between can enhance students’ learning.

Quotation 5: Having sufficient time to study before the exam is very effective and reduced stress and increases efficiency (S7).

Quotation 6: When I have time, I read the question carefully, concentrate better, and memorise the content. When I have enough time to attend to the questions, I am able to concentrate on that topic and keep it in my mind (S4).

Field note 1: (In the OSCE) the station's instructions were not clear enough. Some students had to read the instructions several times. Some asked the rater questions about one of the items in the instruction. This took a lot of time. Moreover, this increased students stress.

Quotation 7: When the content is divided, you can read on a daily basis and this promotes learning, but when you are assessed only at the end of the course, you just cram several days before the exam and forget the content shortly after the exam (S2).

Mechanism of action
According to the findings of this study, we divided the mechanisms by which different elements of assessment system influence learning into those acting ‘before assessment’ and those acting ‘during or after assessment’.

Before assessment
The way assessment impacts learning before the examination can be through driving students to study and get prepared, influencing their study style (superficial, deep or achieving) and determining the content to be studied. While a few students may study even without having to take an examination, most of our participants believed that preparation for examination is a very important driver for studying. Moreover, the assessment system influences the approach students to prepare themselves for the examination. Assessment also determines the time students allocate to their study. Depending on the test format and the perception of the students of the examination type, they may choose to study reference books, lecture notes or simply test items from previous examinations.

Quotation 8: Exam is an instrument to force students to study (S1).

Quotation 9: An exam that contains conceptual questions necessitates deep study to create relationship between different aspects; otherwise, you just memorise the content to answer the question (S2).

Quotation 10: Exams are helpful in that they force students to study; otherwise, students will not (T5).

During and after assessment
Examinations can improve students’ learning through challenging their mind while answering the questions, confirming the significance of the topics, stabilisation of the content studied and providing feedback as well as an opportunity for practice and reflection. Assessment items may require information recovery, integration of various pieces of information and applying the knowledge. As the students challenge their knowledge against the required task, they may deepen their knowledge and at times even correct their understanding of the concepts. Providing the student with feedback about their performance during (especially in clinical and practical assessments) or after assessment usually leads to effective learning, too.

Field note 2: (In the OSCE) sometimes when the student failed to accomplish the required task, the rater explained to the student what he/she was supposed to do. In these cases, the examinee often left the station with satisfaction and thanked the rater.

After the exam, students’ verification of the correct answers and discussing the questions with peers as well as reflection on their own performance can provide learning opportunities for them.

Quotation 11: When the teacher asks an interdisciplinary question that requires deep reflection, if I can answer it, it remains in my mind because I did not only recover information, but created relationships between what I knew to answer the question (S3).

Quotation 12: I think when a question is asked, all learned materials are classified and this facilitates my learning (S4).

Quotation 13: After the exam, we look back on the questions and check the answers. Everybody justifies their responses and we discuss in groups. We may need to go back to the textbooks to find out the reason. I believe these discussions after the exam facilitate learning (S5).

Educational consequences
Different elements of the assessment system work through the aforementioned mechanisms to influence students’ learning in several aspects: learnt content, depth of learning, stability (retention) of learnt content and the feeling resulting from learning (emotional involvement).

Learning content
Usually, part of the content that is being assessed will receive attention from students. Not so infrequently, some learning objectives (such as communication skills, professionalism, etc) are simply ignored in the assessment system due to difficulty of their evaluation. This, in turn, causes the topic to be neglected by the students. In another case, the examination assesses knowledge or skill which is beyond the expectations of that level of training (such as complicated medical cases to be solved by undergraduate medical students). This practice encourages students to learn topics that will not be useful in their future career.

Quotation 14: Some teachers take exams with the same content each year but the questions are different. It this condition, we should learn the content (S6).

Quotation 15: When preparing for an exam, we ask students who have already passed that exam about the topics usually covered in the exam and try to keep that in mind (S8).

Depth of learning
When obtaining good marks in the examination requires mastery of the learning content, students employ a deep learning strategy. However, when there is not a consistent relationship between learning and the examination scores and it is possible to obtain good marks through other methods such as rote memorisation or reviewing previous examination items, students prioritise obtaining better results over deep learning of the content.

Quotation 16: Case-based assessment facilitates deep learning and storage of content and shows us the relevance of basic sciences with clinical sciences (S4).

Quotation 17: I noticed that although I study the content and learn it deeply, I cannot get high marks but a friend of mine who simply memorises the content and practices with sample questions gets higher marks. This forced me to change the way I study to get better results (S6).

Learning retention
As assessment helps student link the new content with previous content, durability of their learning also increases. This effect is believed to be enhanced by frequent assessments. The experience of students showed that when they are assessed in action (eg, while managing a patient at bedside), their learning lasts longer.

Quotation 18: In scenario-based exams, you not only had to study deeply for the exam but also retain that learning for a much longer time (FG).

Emotional involvement
The assessment system and students’ perception of the assessment process influence their satisfaction with their learning experience. This, in turn, impacts the probability of whether they will be encouraged to study further or become disappointed with studying. These feelings may also affect their confidence either positively or adversely.

Quotation 19: I imagine that the exams are only meant to disturb students and they do not have any educational objective. Why should they ask so many questions about, for example, cancer staging from undergraduate medical students. (S1).

Contextual factors
The experiences of participants suggest that the interactions between assessment and learning occur within a set of contextual factors which in turn affects various parts of this model. These include factors related to the organisation such as the facilities and supports available for quality assessment, factors related to the society such as values placed on teaching and assessment which affects professional priorities of the assessor, as well as the experiences and feedbacks conveyed by peers.

Quotation 20: What we hear from our senior schoolmates often discourages us from studying (S2).

Quotation 21: Many students plan to study abroad and so they work hard to obtain high marks and have a favourable transcript. This is more important for them than learning (S8).

Discussion
The purpose of this qualitative study was to explore how assessment affects students’ learning. The core concepts in this study were categorised into four categories: contributing factors, mechanism of action, educational consequences and contextual factors.

Contributing factors
The results of our study revealed that assessors are important and influential elements in the educational impact of assessment. Their motivation to conduct a proper assessment, their attitude towards the assessment, their awareness of the purpose of the assessment, their knowledge and skill to design a particular type of examination, as well as their leniency/stringency can affect students’ learning. This finding is in accordance with several other studies. In a 2005 study by Tiwari et al, the students stated that the efforts they made to prepare for the assessment depended on how stringent the assessor was.22 Al-Kadri et al concluded in their study that students’ and supervisors’ orientation to the assessment process is one of the factors influencing students’ learning from the assessment.14 In addition, Henneman et al considered student-examiner interactions in oral examinations as one of the effective factors in students’ learning during assessment.8 In a recent study by Lörwald et al, it was also suggested that knowledge and attitude of supervisors regarding the mini-CEX and DOPS influence students’ learning.41 The assessor's motivation for assessment has not been explicitly mentioned as an effective factor in previous studies. Although this effect may be exerted indirectly through proper test design and delivery, frequency of raising this topic in our interviews convinced us that it is an important issue that should be dealt with separately.

In this study, the characteristics of the assessee were also identified as other influential elements on the educational consequences of assessment. Their personal characteristics such as level of stress experienced, ambitiousness and competitiveness, their motivation to study and learn, as well as their understanding of what is needed to succeed in the examinations influence the quality and the amount of educational impact of the examination. Some previous studies corroborate our findings. The results of various studies showed that there was a direct correlation between students’ perception of the needs of the examination and the adoption of the strategies and approaches for study.11 21–28 Students’ perception of the test requirements are mentioned in relevant studies as an important influencing factor. The results of our study showed that personal characteristics such as motivation are important factors that affect student learning.

This study also revealed that the characteristics of the assessment method, including assessment format, the content of the assessment, the method of scoring, stakes of the examination, examination duration, the frequency of assessments and the interval between the examinations could impact the students' learning. Many other studies back up the influence of this factor. The results of the study by Al-Kadri et al showed that students' perceptions of assessment, student perception of learning goals and student experience of authentic assessment in a clinical setting are three factors that influence the adoption of learning strategies in students.42 The results of these studies have shown that factors such as the method of assessment, the assessment weight and the time it takes for study affect the choice of the study approach. Also, the fairness of the assessment (the appropriate blueprint design for testing and that the test is based on the goals of learning curriculum) is another factor that can influence the student’s study and learning approach.29 Formative assessments, in comparison to summative assessments, lead to more in-depth learning. Also, continuous assessments increase knowledge and self-awareness among students.24 Most factors except the examination duration have been mentioned in the Cilliers et al model.9 In many studies, the format, frequency and test stake of examination are listed as influencing factors on students learning. In the study by Al-Kadri et al, formative assessments led students to use deeper approaches compared with summative assessments.24 In a study by Opoka et al, students stated that test format had an impact on their learning, and pointed out that the OSCE examination influenced their study behaviours and improved their communicative and procedural skills.43 Also, results of study by Huwendiek et al showed that the use of the key-feature problems to assess clinical reasoning during the clerkship period better motivated students for intense study in comparison to the context-rich single best answer questions.44


In a study by Henneman et al, students indicated oral examinations, in which they were requested to explain mechanisms instead of facts, led them towards deeper understanding. They also stated that assessments that were summative and influenced pass/fail decisions led to more learning activities, especially in the near-test days.8 Performing the test several times throughout the term has a positive effect on student performance in the final test.45 In addition, the test has been shown to affect learning and retention of the content more than the re-reading of the content.45 46 The results of a study by Agarwal et al showed that taking the initial test after the study leads to a longer-term retention.47 Cilliers et al described the factors influencing students' learning in summative assessments in two categories: task demands and system design. Task demands included task type, assessment criteria, nature of assessment material, past papers, cues from lecturers, cues from informal communication networks between students and lack of cues. It was also mentioned that system design influences the learning resulting from students' assessment through pattern of scheduling and imminent and prevailing workload.9 These factors have also been elicited in our study.

Mechanism of action
We divided the mechanisms through which assessment impacts learning as those that are effective before the assessment and those that are operational during and after the assessment. Participants’ experiences indicated that before assessment, the examination influences students’ learning by driving them to study, influencing the adoption of the appropriate learning approach and selecting the content to be studied. Moreover, several mechanisms helped students learn during and after the assessment. These included the mental challenge and practice while answering the test questions, active retrieval of the content of the study in mind, adding up the new content to the previous knowledge, the opportunity to receive feedback and provision of opportunities for exercise and reflection. Similar findings have been reported in previous studies. The results of Tiwari et al showed that students in the clinical phase learnt the material that was assessed. Furthermore, they found that the assessment not only tells them what to learn, but also tells them how to learn and what strategies to use for studying and learning.22 The results of Cobb et al showed that students’ choice of approach to study is influenced by the examination.21 Moreover, Henneman et al and Opoka et al demonstrated that assessment motivates and induces students to study.8 43 In a study by Larsen et al, the authors suggested that assessment indirectly increases the incentive to study and urges students to employ more effective strategies for learning.48 A study by Cilliers et al revealed that effective factors of assessment influence students learning through the appraise of impact and response, such as the student's understanding of self-efficacy and their beliefs about assessment and their motivation to respond to these beliefs.9 The results of previous studies on the effect of assessment on students' learning during and after the test were also somewhat similar. There are few studies in relation to the impact of assessment on learning, during the test. These studies have addressed the process of thinking during the test with similar terms, such as reorganisation. Nevo (1995) and Struyf et al
49 stated that during the assessment, students should reorganise their knowledge. They encounter a new issue that has not been addressed in their study and it is necessary to consider the relationship between the various aspects of the problem. Assessment stimulates the thinking process and promotes using higher levels of cognition, hence providing the students with a rich learning experience.2 In many studies the role of feedback after the assessment on student learning has been discussed. Meanwhile, the results of our study showed that, in addition to postexamination feedback, feedback during practical and clinical assessment can also greatly affect students’ learning. Larsen et al maintained that feedback improves learning through correction of errors and fixing correct answers in the mind. The timing of feedback is also very important. Immediately feedback may lead to deeper learning in the student than delayed feedback.48 After submitting feedback to the student, it is the appraisal of feedback which leads to the educational impact of the assessment.41 The study of Dijksterhuis et al showed that that feedback does not always influence learning positively. They suggested that the credibility of the feedback provider and the feedback content are very important in learning which results from formative assessment.50 Olupeliyawa et al also showed that providing feedback in workplace-based assessments has an important role in developing students’ competencies.13 The results of the study by Roediger and Karpicke (2006a) showed that testing indirectly affects students’ future studies through feedback received.51 Agarwal et al conducted a study and indicated that the students who were given feedback after the initial test had better performance than the other students who were not given feedback on their performance in the final test.47 Peer feedbacks have also been regarded as a good source for reflection and improvement of competencies, especially those related to team work.8 Typically, students reflect on the content and learning processes after the assessment.2 Students use their cognitive strategies to compare their problem solving processes with the teacher and other students at reflection process.49 The results of McKenzie et al showed that assessment, along with feedback based on the Pendleton's method, prompted students to reflect on their performance and helped them become aware of their strengths and limitations.52 In this regard, feedback quality is very important.

Educational consequences
Our model suggests that the final product of the actions and interactions of the contributing factors, through the aforementioned mechanisms, is a change in learning content, depth of learning, the durability of learning and ultimately the students’ emotional involvement. These are actually the observable consequences driven from assessment. Similar findings in the literature were found. Result of study Kromann et al showed final test for resuscitation skills at medical students increased the learning outcome of them, compared with students who took the time to spend practicing resuscitation skill.53 In a study by Opoka et al to assess the perception of students, most postgraduate students believed that OSCE improved their communicative and procedural skills.43 In the study by McKenzie et al, students’ experiences showed that the assessment of their procedural skills and the feedback provided to them after that increased their confidence in conducting the procedure at the patient’s bedside.52 The study by Cilliers et al divided the effects of learning on summative assessment into two categories of the nature of cognitive processing activities and metacognition regulatory activities.9


Contextual factors
In this study, the contextual factors such as peers, family, institution facilities, community values and students’ extracurricular activities were found to affect all of the mentioned components in the model. The characteristic of an assessment method is not inherent to that method, but depends on how and on what context the assessment is carried out. Therefore, similar methods can have different educational impacts depending on how they are used and the context in which they take place.5 The results of the study by Cilliers et al showed that students in the informal communication (student grapevine) received cues about the assessment from peers and senior students. These cues affect cognitive and metacognitive activities (such as the amount of effort, the source of the study, the content studied and the strategy used to study), and, consequently, their learning.9 In addition, the beliefs of the community, peers and the views of other people who influence the student can affect student behaviour.4 The results of the Alkharusi study showed that self-efficacy levels of students is directly correlated with students’ perceptions of assessment environment, and found that perceived assessment environment leads to an increase in students’ self-efficacy, while harsh assessment environments reduce the level of self-efficacy.54 Al-Kadri et al considered peers’ feedback very effective in selecting and using learning approaches.14 The study by Heeneman et al also showed that the feedback provided by peers affects students’ reflection on their performance.8


Cilliers et al presented pre-assessment learning effects of summative assessments within a framework. In this study, the factors associated with the examination, including the assessment criteria, the content of the assessment, previous tests and clues that the student received from their peers or in the classroom on the examination, were related to the assessment factors that could affect student learning. In addition, the timetable of the examinations, the volume of the content being assessed and the appropriateness of the examination time are also influential in this regard. Effective factors influence the cognitive and metacognitive activities through the assessment of impact and response, as well as the student's understanding of self-efficacy and his beliefs about his assessment and his motivation to respond to these beliefs. Cognitive and metacognitive activities of the study were the level of learning, the choice of content and reference study, the amount of effort that is being used, the monitoring, the setting of learning strategies and learning retention. Their emphasis was on the impact of pre-assessment learning.9 However, in the present study, the effect of evaluation is also considered during and after the test. Furthermore, in spite of the low number of formative assessment in our setting, this study attempted to assess both the formative and summative assessments.

This study developed a comprehensive, yet easy to understand, model to illustrate the educational consequences of assessment by categorising the findings into the four components of contributing factors, mechanisms of action, educational consequences and contextual factors.

Moreover, we tried to explore the attitudes and experiences of diverse groups of stakeholders, students and faculty members, and multisources of data gathering were used. There are several limitations to this study. Voluntary participation in the study and the setting of a single institution may restrict the transferability of our results to other contexts. Although the study aimed at both formative and summative assessments, since the participants’ experiences in formative assessments were limited, the results of the study seem to focus more on summative assessment. Moreover, we studied undergraduate students in this study. For further validation and generalisation of this model, it is suggested that future studies be conducted on how assessment affects postgraduate students’ learning.

Conclusion
This study suggested a model for understanding the elements of the assessment which, within the context, affect learning, the mechanisms through which they impart their effects and the final outcomes obtained.

Supplementary Material
Reviewer comments
 Author's manuscript
 We would like to express our sincere gratitude to all participants in this study.

Contributors: MJ brought up the original idea and together with AKM designed the project. MJ, AKM and HRK contributed to the development of the study methodology. AKM and MJ had full access to all of the data in the study and are responsible for the integrity of the data. AKM collected and analysed the data and is responsible for the accuracy of statistical analysis. MJ, JN and HRK made substantial contribution to the interpretation of findings. AKM wrote the first version of the article. MJ, MS, HRK and JN critically revised the first draft. All authors approved the final version and agreed to be responsible for all aspects of the work. MJ supervised the whole process.

Funding: This project was part of a PhD thesis supported and funded by Tehran University of Medical Sciences.

Competing interests: None declared.

Patient consent for publication: Not required.

Ethics approval: Institutional Review Board (No. 20. 2015-3-11)

Provenance and peer review: Not commissioned; externally peer reviewed.

Data availability statement: Data are available in a public, open access repository. Data are available upon reasonable request.
==== Refs
References
1. 
Norcini J , Anderson B , Bollela V , et al 
Criteria for good assessment: consensus statement and recommendations from the Ottawa 2010 conference . Med Teach 
2011 ;33 :206 –14 . 10.3109/0142159X.2011.551559 
21345060 
2. 
Dochy F , Segers M , Segers M , et al 
Struyven, KAssessment engineering: Breaking down barriers between teaching and learning, and assessment : Boud D , Falchikov N  , Rethinking assessment in higher education: learning for the longer term . Oxford : Routledge , 2007 : 87 –100 .
3. 
Schuwirth LWT , van der Vleuten CPM  
Different written assessment methods: what can be said about their strengths and weaknesses? 
Med Educ 
2004 ;38 :974 –9 . 10.1111/j.1365-2929.2004.01916.x 
15327679 
4. 
Cilliers FJ , Schuwirth LW , Adendorff HJ , et al 
The mechanism of impact of summative assessment on medical students’ learning . Adv in Health Sci Educ 
2010 ;15 :695 –715 . 10.1007/s10459-010-9232-9 

5. 
van der Vleuten CPM , Schuwirth LWT  
Assessing professional competence: from methods to programmes . Med Educ 
2005 ;39 :309 –17 . 10.1111/j.1365-2929.2005.02094.x 
15733167 
6. 
Schuwirth LWT , Van der Vleuten CPM  
Programmatic assessment: from assessment of learning to assessment for learning . Med Teach 
2011 ;33 :478 –85 . 10.3109/0142159X.2011.565828 
21609177 
7. 
Raupach T , Brown J , Anders S , et al 
Summative assessments are more powerful drivers of student learning than resource intensive teaching formats . BMC Med 
2013 ;11 :61
10.1186/1741-7015-11-61 
23497243 
8. 
Heeneman S , Oudkerk Pool A , Schuwirth LWT , et al 
The impact of programmatic assessment on student learning: theory versus practice . Med Educ 
2015 ;49 :487 –98 . 10.1111/medu.12645 
25924124 
9. 
Cilliers FJ , Schuwirth LWT , Herman N , et al 
A model of the pre-assessment learning effects of summative assessment in medical education . Adv in Health Sci Educ 
2012 ;17 :39 –53 . 10.1007/s10459-011-9292-5 

10. 
Epstein RM  
Assessment in medical education . N Engl J Med 
2007 ;356 :387 –96 . 10.1056/NEJMra054784 
17251535 
11. 
Broekkamp H , Van Hout-Wolters BHAM  
Students’ adaptation of study strategies when preparing for classroom tests . Educ Psychol Rev 
2007 ;19 :401 –28 . 10.1007/s10648-006-9025-0 

12. 
Al-Kadri HM , Al-moamary MS , roberts C , et al 
Exploring assessment factors contributing to students' study strategies: literature review . Med Teach 
2012 ;34 :S42 –50 . 10.3109/0142159X.2012.656756 
22409191 
13. 
Olupeliyawa A , Balasooriya C , Hughes C , et al 
Educational impact of an assessment of medical students' collaboration in health care teams . Med Educ 
2014 ;48 :146 –56 . 10.1111/medu.12318 
24528397 
14. 
Al-Kadri HM , Al-Kadi MT , Van Der Vleuten CPM  
Workplace-based assessment and students’ approaches to learning: a qualitative inquiry . Med Teach 
2013 ;35 :S31 –8 . 10.3109/0142159X.2013.765547 
23581894 
15. 
Koh LC  
Refocusing formative feedback to enhance learning in PRE-REGISTRATION nurse education . Nurse Educ Pract 
2008 ;8 :223 –30 . 10.1016/j.nepr.2007.08.002 
17959416 
16. 
Rushton A  
Formative assessment: a key to deep learning? 
Med Teach 
2005 ;27 :509 –13 . 10.1080/01421590500129159 
16199357 
17. 
Baeten M , Dochy F , Struyven K  
Students’ approaches to learning and assessment preferences in a portfolio-based learning environment . Instr Sci 
2008 ;36 :359 –74 . 10.1007/s11251-008-9060-y 

18. 
Sluijsmans DMA , Brand-Gruwel S , van Merriënboer JJG  
Peer assessment training in teacher education: effects on performance and perceptions . Assess Eval High Educ 
2002 ;27 :443 –54 . 10.1080/0260293022000009311 

19. 
Tillema HH  
Portfolios as developmental assessment tools . International Journal of Training and Development 
2001 ;5 :126 –35 . 10.1111/1468-2419.00127 

20. 
Leung SF , Mok E , Wong D  
The impact of assessment methods on the learning of nursing students . Nurse Educ Today 
2008 ;28 :711 –9 . 10.1016/j.nedt.2007.11.004 
18164105 
21. 
Cobb KA , Brown G , Jaarsma DADC , et al 
The educational impact of assessment: a comparison of DOPS and MCQs . Med Teach 
2013 ;35 :e1598 –607 . 10.3109/0142159X.2013.803061 
23808609 
22. 
Tiwari A , Lam D , Yuen KH , et al 
Student learning in clinical nursing education: perceptions of the relationship between assessment and learning . Nurse Educ Today 
2005 ;25 :299 –308 . 10.1016/j.nedt.2005.01.013 
15896415 
23. 
Shen J , Hiltz SR , Bieber M  
Learning strategies in online collaborative examinations . IEEE Trans Prof Commun 
2008 ;51 :63 –78 . 10.1109/TPC.2007.2000053 

24. 
Al Kadri HMF , Al-Moamary MS , van der Vleuten C  
Students' and teachers' perceptions of clinical assessment program: a qualitative study in a PBL curriculum . BMC Res Notes 
2009 ;2 :263
10.1186/1756-0500-2-263 
20030842 
25. 
Segers M , Nijhuis J , Gijselaers W  
Redesigning a learning and assessment environment: the influence on students' perceptions of assessment demands and their learning strategies . Studies in Educational Evaluation 
2006 ;32 :223 –42 . 10.1016/j.stueduc.2006.08.004 

26. 
Gijbels D , Segers M , Struyf E  
Constructivist learning environments and the (im)possibility to change students’ perceptions of assessment demands and approaches to learning . Instructional Science 
2008 ;36 :431 –43 . 10.1007/s11251-008-9064-7 

27. 
Nijhuis JFH , Segers MSR , Gijselaers WH  
Influence of redesigning a learning environment on student perceptions and learning strategies . Learn Environ Res 
2005 ;8 :67 –93 . 10.1007/s10984-005-7950-3 

28. 
Segers M , Martens R , Van den Bossche P  
Understanding how a case-based assessment instrument influences student teachers’ learning approaches . Teach Teach Educ 
2008 ;24 :1751 –64 . 10.1016/j.tate.2008.02.022 

29. 
Al Kadri HMF , Al-Moamary MS , Elzubair M , et al 
Exploring factors affecting undergraduate medical students' study strategies in the clinical years: a qualitative study . Adv Health Sci Educ Theory Pract 
2011 ;16 :553 –67 . 10.1007/s10459-010-9271-2 
21207243 
30. 
Corbin J , Strauss A  
Basics of qualitative research: techniques and procedures for developing Grounded theory . 3rd edn 
USA : Sage , 2008 .
31. 
Baptiste I  
Qualitative data analysis: common phases, strategic differences . Forum: qualitative social research 
2001 ;2 :Art2.
32. 
Kolb SM  
Grounded theory and the constant comparative method: valid research strategies for educators . Journal of Emerging Trends in Educational Research and Policy Studies 
2012 ;3 :83 –6 .
33. 
Bitsch V  
Qualitative research: a grounded theory example and evaluation criteria . J Agribus 
2005 ;23 .
34. 
Bekhet AK , Zauszniewski JA  
Methodological triangulation: an approach to understanding data . Nurse Res 
2012 ;20 :40 –3 . 10.7748/nr2012.11.20.2.40.c9442 
23316537 
35. 
Chong C-H , Yeo K-J  
An overview of grounded theory design in educational research . Asian Soc Sci 
2015 ;11 
10.5539/ass.v11n12p258 

36. 
Halcomb EJ , Gholizadeh L , DiGiacomo M , et al 
Literature review: considerations in undertaking focus group research with culturally and linguistically diverse groups . J Clin Nurs 
2007 ;16 :1000 –11 . 10.1111/j.1365-2702.2006.01760.x 
17518876 
37. 
Carlsen B , Glenton C  
What about N? A methodological study of sample-size reporting in focus group studies . BMC Med Res Methodol 
2011 ;11 :26
10.1186/1471-2288-11-26 
21396104 
38. 
Phillippi J , Lauderdale J  
A guide to field notes for qualitative research: context and conversation . Qual Health Res 
2018 ;28 :381 –8 . 10.1177/1049732317697102 
29298584 
39. 
Schwandt TA , Lincoln YS , Guba EG  
Judging interpretations: but is it rigorous? trustworthiness and authenticity in naturalistic evaluation . New Dir Eval 
2007 ;2007 :11 –25 . 10.1002/ev.223 

40. 
Speziale HS , Streubert HJ , Carpenter DR  
Qualitative research in nursing: advancing the humanistic imperative . Lippincott Williams & Wilkins , 2011 .
41. 
Lörwald AC , Lahner F-M , Greif R , et al 
Factors influencing the educational impact of Mini-CEX and DOPS: a qualitative synthesis . Med Teach 
2018 ;40 :414 –20 . 10.1080/0142159X.2017.1408901 
29188739 
42. 
Al Kadri HM , Al-Moamary MS , Magzoub ME , et al 
Students' perceptions of the impact of assessment on approaches to learning: a comparison between two medical schools with similar curricula . Int J Med Educ 
2011 ;2 :44 –52 . 10.5116/ijme.4ddb.fc11 

43. 
Opoka R , Kiguli S , Ssemata A , et al 
Perceptions of postgraduate trainees on the impact of objective structured clinical examinations on their study behavior and clinical practice . Adv Med Educ Pract 
2015 ;6 :431
10.2147/AMEP.S79557 
26082673 
44. 
Huwendiek S , Reichert F , Duncker C , et al 
Electronic assessment of clinical Reasoning in clerkships: a mixed-methods comparison of long-menu key-feature problems with context-rich single best answer questions . Med Teach 
2017 ;39 :476 –85 . 10.1080/0142159X.2017.1297525 
28281369 
45. 
McDaniel MA , Anderson JL , Derbish MH , et al 
Testing the testing effect in the classroom . Eur J of Cogn Psychol 
2007 ;19 :494 –513 . 10.1080/09541440701326154 

46. 
Carpenter SK  
Cue strength as a moderator of the testing effect: the benefits of elaborative retrieval . J Exp Psychol 
2009 ;35 :1563 –9 . 10.1037/a0017021 

47. 
Agarwal PK , Karpicke JD , Kang SHK , et al 
Examining the testing effect with open- and closed-book tests . Appl Cogn Psychol 
2008 ;22 :861 –76 . 10.1002/acp.1391 

48. 
Larsen DP , Butler AC , Roediger III HL  
Test-Enhanced learning in medical education . Med Educ 
2008 ;42 :959 –66 . 10.1111/j.1365-2923.2008.03124.x 
18823514 
49. 
Struyf E , Vandenberghe R , Lens W  
The evaluation practice of teachers as a learning opportunity for students . Stud in Educ Eval 
2001 ;27 :215 –38 . 10.1016/S0191-491X(01)00027-X 

50. 
Dijksterhuis MGK , Schuwirth LWT , Braat DDM , et al 
A qualitative study on trainees’ and supervisors’ perceptions of assessment for learning in postgraduate medical education . Med Teach 
2013 ;35 :e1396 –402 . 10.3109/0142159X.2012.756576 
23600668 
51. 
Toppino TC , Cohen MS  
The testing effect and the retention interval: questions and answers . Exp Psychol 
2009 ;56 :252 –7 . 10.1027/1618-3169.56.4.252 
19439397 
52. 
McKenzie S , Burgess A , Mellis C  
Interns reflect: the effect of formative assessment with feedback during pre-internship . Adv in Med Educ and Pract 
2017 ;8 :51 –6 . 10.2147/AMEP.S114480 
28138270 
53. 
Kromann CB , Jensen ML , Ringsted C  
The effect of testing on skills learning . Med Educ 
2009 ;43 :21 –7 . 10.1111/j.1365-2923.2008.03245.x 
19140995 
54. 
Alkharusi H  
A multilevel linear model of teachers’ assessment practices and students’ perceptions of the classroom assessment environment . Procedia Soc Behav Sci 
2010 ;5 :5 –11 . 10.1016/j.sbspro.2010.07.041

