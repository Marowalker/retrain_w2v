
==== Front
ACS Omega
ACS Omega
ao
acsodf
ACS Omega
2470-1343 American Chemical Society 

10.1021/acsomega.9b04104
Article
Tracking the Chemical Evolution of Iodine Species
Using Recurrent Neural Networks
Bilbrey Jenna A. * Marrero Carlos Ortiz  Sassi Michel  Ritzmann Andrew M.  Henson Neil J.  Schram Malachi  Pacific Northwest National Laboratory, 902 Battelle Boulevard, P.O. Box 999, Richland, Washington 99352, United States
* E-mail: jenna.bilbrey@pnnl.gov. Phone: (509)372-4256.
28 02 2020 
10 03 2020 
5 9 4588 4594
02 12 2019 14 02 2020 Copyright © 2020 American Chemical Society2020American Chemical SocietyThis is an open access article published under an ACS AuthorChoice License, which permits copying and redistribution of the article or any adaptations for non-commercial purposes.

We
apply recurrent neural networks (RNNs) to predict the time evolution
of the concentration profile of multiple species resulting from a
set of interconnected chemical reactions. As a proof of concept of our approach, RNNs were trained on
a synthetic dataset generated by solving the kinetic equations of
a system of aqueous inorganic iodine reactions that can follow after
nuclear reactor accidents. We examine the minimum dataset necessary
to obtain accurate predictions and explore the ability of RNNs to
interpolate and extrapolate when exposed to previously unseen data.
We also investigate the limits of our RNN by evaluating the robustness
of the training initialization on our dataset.

document-id-old-9ao9b04104document-id-new-14ao9b04104ccc-price
==== Body
Introduction
Radioactive iodine-131
(131I) is a byproduct of uranium
fission. Under normal operation of a nuclear power plant, little to
no iodine is released into the environment; it is either contained
in the spent fuel or scrubbed by filtration systems. However, if an
incident occurs, such as when the To̅hoku earthquake and tsunami
hit the Fukushima Daiichi Nuclear Power Plant in 2011, radioiodine
can be released into the environment.1,2 The half-life
of 131I is 8.0262 ± 0.0006 days,3 during which it can undergo a number of reactions in the
atmosphere, including those in the gaseous and aqueous phases as well
as solid-state reactions with particulates in the atmosphere that
likely result from the incident. After nuclear accidents, 131I is commonly observed in the environment. Takeyasu et al. monitored
the fallout of radionuclides from the Fukushima accident and found 131I to be present in the highest concentration among all radionuclides.4

Radioiodine presents health problems because
it can enter the body
through numerous mechanisms. Radioiodine in the aqueous phase can
enter local water sources and be imbibed directly or taken up by plants
and animals, where it is then passed to the population. In the gaseous
phase, radioiodine can enter the bloodstream through inhalation. Once
in the body, radioiodine concentrates in the thyroid gland and has
the potential to cause thyroid cancer.5−7 For example, after the
accident at the Chernobyl Nuclear Power Plant in 1986, those that
were not impacted by direct radiation exposure were affected by 131I released from the explosion, mainly through contact with
contaminated vegetable and dairy products.8 After the Fukushima accident, 131I was detected in human
breast milk, even when only mild environmental 131I pollution
was present.9 Recently, researchers in
Japan have detected high rates of thyroid cancer in young individuals,
thought to have resulted from radiation exposure after the Fukushima
accident.10

Radioiodine can react
with hydroxyl radicals present in the atmosphere
to form an array of species. A number of reports have examined the
multitude of species generated in such accidents.11−15 In this work, we adopt the library of iodine reactions
in containment (LIRIC) created by Wren and Ball, which provides a
comprehensive mechanistic model for the behavior of iodine in containment
under nuclear reactor accident conditions.12 Their library consists of nearly 200 reactions and includes chemical
reactions, surface interactions, and mass transport processes, along
with their respective rate constants. Notably, LIRIC does not contain
nuclear decay rates. As our goal is to explore the utility of deep
learning methods in tracking complex chemical evolutions, we do not
consider the added complexity of nuclear decay when determining the
fate of iodine, as the time scale of the studied system of reactions
is much shorter than the half-life of 131I.

Deep
learning methods are gaining popularity for the task of forecasting
time series data. Recurrent neural networks (RNNs), in particular,
are powerful tools for predicting large input sequences of variable
lengths,16 exemplified by complex chemical
reactions. Such networks are commonly applied to language17 and image18 processing.
Long short-term memory (LSTM) is a type of RNN that better captures
long-term dependencies within the data by passing a sequence of hidden
states. LSTM employs input, forget, and output gates to control the
information that is passed to the successive cell through the hidden
state, allowing important information to pass through cells unchanged.
Notably, this gating mechanism overcomes the vanishing and exploding
gradient problem encountered when attempting to learn long sequences.19,20

When applied to chemistry problems, RNNs are often used for
property
prediction,21,22 product prediction,23,24 or molecular design in a specific region of chemical space,25−27 often using a graph or textual representation of the molecule under
study. Here, we apply RNNs to a chemical kinetics problem, namely,
predicting the chemical evolution of species as a reaction progresses.
We combine kinetic models from two systems previously published in
the literature to examine the ability of RNNs to predict the chemical
evolution of iodine species in a supposed nuclear reactor incident.
We combine a previously published model for the radiolysis of water28,29 to a closed subset of the LIRIC model that involves aqueous-phase
reactions of inorganic iodine species. The RNN is agnostic to the
actual species under question and instead utilizes concentration data
in the form of a time series. The RNN predicts the concentrations
of all species as an ensemble, which allows for correlations between
the different species to be leveraged when making predictions. In
this report, we show that RNNs are able to produce highly accurate
predictions of the evolution of multiple chemical species with concentrations
inside the bounds of those in the training set, while the predictions
quickly worsen when the concentrations fall too far outside of those
bounds. We also investigate the network variability and sampling size
necessary to achieve good predictions. We find that the variability
in the sampling choice increases as the size of the training set decreases,
indicating that the careful selection of samples is necessary when
training networks on low amounts of data.

Methods
Data Generation
In 2001, Wren and Ball published a
comprehensive kinetic model called LIRIC that describes the behavior
of iodine under conditions related to incidents at nuclear power plants.12 The same year, Pastina and LaVerne published G values for the radiolysis of water with γ-rays,29 and a few years later, Poinssot et al. published
a report for the European Commission with a complete kinetic model
for radiolytic reactions in pure water.28 To mimic iodine contamination in a water coolant system at a nuclear
power plant, we merged these models, using the full water radiolysis
model and a closed subset of the LIRIC model, by first evolving the
water radiolysis model and applying the outcome as an input to the
iodine model. Note that several of the rate constants in the LIRIC
paper were altered; the full list of reactions and rate constants
used in this work is given in the Supporting Information (see Table S1).

When exposed to ionizing radiation, water
decomposes into a number of ionic and excited states, which quickly
decompose into radical and molecular species. H, OH•, H2, and solvated electrons are formed within a few picoseconds;
short-term products, additional radicals, and primary molecular products
(H2, H2O2, etc.) are formed within
microseconds; and long-term products (secondary molecular products,
such as O2) can take up to an hour to form. Evolving the
water radiolysis model over 10,000 s (approximately 2.8 h) starting
from bulk water results in 2.42 × 10–7 M H+, 1.35 × 10–5 M H2, 1.80
× 10–9 M HO2, 7.02 × 10–6 M H2O2, 3.09 × 10–6 M O2, 2.01 × 10–7 M O2•–, 1.84 × 10–10 M OH•, and 4.15 × 10–8 M
OH–. Note that concentrations below 0.1 nM were
assumed to be 0. These values were input into the iodine kinetic model
along with various concentrations of I2 and I•, depending on the dataset being generated. The system was integrated
over 99 time points from 1 × 10–5 to 0.04 s
to capture the full dynamics of the system. All kinetics equations
were integrated using ChemPy.30

A
training set and four test sets were generated for use with our
RNN using the integration method describe above. Of the 12 species
with non-zero concentrations resulting from the integration of the
kinetic model of aqueous-phase inorganic iodine reactions, H2O, H2O2, and O2 were not included
because their concentrations did not change significantly during the
reaction and thus were unnecessary to track, leaving nine species
for our RNN to learn. In the training set, the initial concentration
of I2 ranged linearly between 1.00 × 10–7 and 5.00 × 10–7 M, while that of I• ranged linearly between 1.00 × 10–8 and 5.00
× 10–8 M. Thirty-two concentrations were used
for each species, giving a total of 1024 combinations. These 1024
simulations comprised our training set, 10% of which were used for
validation during training of the network.

Our first test set
was generated to examine the ability of our
network to interpolate within the bounds of the initial concentrations
in our training set. For this, concentrations between those in the
training set were generated. In the training set, each concentration
differed by 0.12 × 10–7 or 10–8 M, such that the initial concentrations in the training set were
shifted by 0.06 × 10–7 or 10–8 M to force the initial concentrations to lie exactly between those
in the training set, giving a total of 961 simulations. A second test
set was generated to examine the ability of our network to extrapolate
outside the bounds of our training data. In this set, the concentrations
of I2 ranged from 1.00 × 10–9 to
1.00 × 10–7, while those of I• ranged from 1.00 × 10–10 to 1.00 × 10–8, again giving a total of 961 simulations. Two more
test sets were generated, in which the initial I2 concentrations
were within the limits of the training set, while those of I• were outside those limits, and vice versa. Table 1 shows the ranges of concentrations for each
dataset, along with the number of simulations. Separating the test
sets in such a manner allowed us to examine the limits of our network
more thoroughly.

Table 1 Dataset Notations, Number (N) of Simulations, and Concentration Ranges Used in This
Worka
dataset	N	[I2]min	[I2]max	[I•]min	[I•]max	
training
set	1024	1.00 × 10–7	5.00 × 10–7	1.00 × 10–8	5.00 × 10–8	
I2 (I) I• (I)	961	1.06 × 10–7	4.93 × 10–7	1.06 × 10–8	4.93 × 10–8	
I2 (I) I• (E)	930	1.06 × 10–7	4.93 × 10–7	1.00 × 10–10/5.13 × 10–8	9.34 × 10–9/7.00 × 10–8	
I2 (E) I• (I)	930	1.00 × 10–9/5.13 × 10–7	9.34 × 10–8/7.00 × 10–7	1.06 × 10–8	4.93 × 10–8	
I2 (E) I• (E)	900	1.00 × 10–9/5.13 × 10–7	9.34 × 10–8/7.00 × 10–7	1.00 × 10–10/5.13 × 10–8	9.34 × 10–9/7.00 × 10–8	
a I indicates that
the initial concentrations are inside the bounds of the training set,
while E indicates that the initial concentrations
are outside the bounds. Note that the values in the E datasets encompass a range below the values in the training set
and a range above the values in the training set. The minimum and
maximum for both ranges are given.

Network Structure
We apply a feed-forward network in
which the simulation sequences of normalized concentrations pass from
the input layer through a hidden LSTM layer to a fully connected output
layer to produce the normalized concentrations of all species simultaneously
at the next time step (Figure 1). In both the LSTM and fully connected layers, the input
data undergo a series of linear transformations, the weights of which
are learned during training, which are then passed through a nonlinear
activation function. The LSTM layer also applies a series of gates
designed to selectively retain and forget certain information from
previous time steps; this gating is also learned during training.
Given that the training data were generated by solving a set of coupled
ordinary differential equations (ODEs), the process of training our
network is an attempt to approximate the flow map of the set of ODEs.31 Note that the process of training our network
required no a priori knowledge of the ODEs involved in generating
the data.

Both the input and output of our network are a series
of concentrations, the values of which vary from 10–10 to 10–7. Such large variation can decrease the
performance of deep learning algorithms, and so we applied min–max
scaling to the concentrations before training to ensure that the concentrations
of all species were within the same order of magnitude (see Figure 2). During training,
the rectified linear unit (ReLU) was used as the activation function
in both the LSTM and fully connected layers to provide nonlinearity
and prevent the occurrence of concentrations with negative values.
ReLU is a non-negative, piecewise linear activation function that
has become popular in recent years for its ability to produce sparse
features and further combat the possibility of vanishing gradients
during training.32

Figure 1 Diagram of our RNN. The
input layer, which consists of a vector
of the normalized concentrations of all species at t = 0, is shown in yellow. The LSTM layer, which has a total of 512
units, is shown in green; the gray arrows represent the passing of
the hidden state h between LSTM units. The output
layer, which is the vector of the predicted normalized concentrations
of all species at t = 1, is shown in red. The matrices
resulting from the LSTM and output layers pass through the ReLU activation
function.

Figure 2 Example simulation of inorganic iodine reactions
coupled with water
radiolysis (top). In the training set, I2 and I• are given initial concentrations between 1.00–5.00 ×
10–7 M and 1.00–5.00 × 10–8 M, respectively. Any species with concentrations below 0.1 nM over
all time steps are assumed to be 0 and are not tracked by the RNN.
The remaining concentrations are normalized by min–max scaling
to input into the RNN (bottom).

At the beginning of training, the network parameters are set to
an initial state. A poor initialization can impede the training of
a highly nonlinear system; therefore, we used the popular initialization
scheme outlined by Glorot and Bengio, where the initial values for
the weights are drawn from a uniform distribution.32 Because this initialization scheme is non-deterministic,
we set random seeds to ensure reproducible results. Furthermore, to
ensure that the final trained network is not an artifact of initialization,
we trained the network 16 times with different random seeds. All networks
were trained until convergence—that is, we stopped training
once no improvement in the loss of the validation set was seen over
100 consecutive epochs or once a predefined number of maximum epochs
was reached. All networks were optimized using the Adam algorithm.33 The initial learning rate was set to 0.001 and
allowed to dynamically decrease to a minimum of 0.0001 based on the
validation loss; the learning rate was decreased by a factor of 0.2
after the validation loss remained steady for 20 consecutive epochs.

Because the network forecasts only a single time step ahead,
predictions
are made iteratively to derive the entire chemical evolution. Forecasts
from each single time step are added to the growing evolution, which
is fed back into the network to make the prediction at the next time
step. It is expected that this process may lead to compounding errors.

Results and Discussion
Network Performance
We first trained
a single RNN on
all 1024 simulations in our training set. The trained network was
then used to predict the chemical evolutions in our four test sets. Figure 3 (left) shows an
example prediction on the interpolation test set. One can see that
the predictions, represented by dashed lines, almost exactly overlay
the integrated model values, represented by solid lines. The error
in the predictions over the entire interpolation test set is shown
in Figure 3 (right).
The errors for all species over all time steps are approximately two
orders of magnitude smaller than the values of the concentrations,
indicating the excellent interpolative ability of our RNN.

Figure 3 (Left) Example
interpolative prediction from an RNN trained on
1024 simulations. The solid lines are the integrated model values
(ground truth), while the dashed lines represent the RNN predictions.
(Right) Mean errors in the interpolative predictions from the RNN
trained on 1024 simulations. The solid line represents the mean prediction
at each time step, while the shaded region represents the standard
deviation from the mean for each test simulation.

We next tested the extrapolative ability of our RNN. The mean error
begins an order of magnitude higher than that of the interpolation
test set and precipitously increases in the negative direction starting
at 3 × 10–2 s. However, this large error is
not consistent across the test set. The extremes of the extrapolation
test set (i.e., very high and very low concentrations of both species)
give much higher errors than the rest of the set. This is to be expected
given that the network was not trained on data that represent this
region. Removing the 200 simulations with the highest concentration
and the 200 with the lowest, leaving 500 simulations in the extrapolation
test set, prevents the error from exploding as time progresses (see Figure S2). Therefore, our RNN has the ability
to extrapolate up to a point, after which the predictions catastrophically
fail.

To gain a holistic understanding of the error in our predictions,
we use the symmetric mean absolute percentage error (sMAPE), which
was previously shown to be a good descriptor for time series problems,
though it should be noted that large positive errors are penalized
more than large negative errors34,35  where k is the forecasting
horizon (98 in this case), Yt are the concentrations at time t produced
by integrating the kinetics equations (i.e., the ground truth), and Ŷt are the predictions
at time t. The sMAPE expressed in this way has a
lower bound of 0% and upper bound of 200%.

Table 2 gives the
sMAPEs of the predictions of each species in all four test sets. All
but two species have low sMAPEs. Notably, the sMAPEs of H+ and IO– are above 50%. Both species have very
low maximum concentrations of less than 1.8 × 10–10 over the entire training set. The comparatively low concentrations
are why the comparatively large errors do not show up in strict analysis
of the errors, but do appear when analyzed by the sMAPE.

Table 2 sMAPE of the Predictions of Each Species
for Four Distinct Test Sets Made by the RNN Trained on 1024 Simulationsa
sMAPE (%)	
species	I2 (I) I• (I)	I2 (I) I• (E)	I2 (E) I• (I)	I2 (E) I• (E)	
H+	52.41 ± 5.87	53.77 ± 7.07	100.43 ± 34.41	105.73 ± 38.92	
HOI	0.69 ± 0.98	1.34 ± 2.40	42.63 ± 53.71	51.60 ± 55.22	
I–	0.32 ± 0.38	0.46 ± 0.79	32.91 ± 46.86	38.13 ± 48.35	
I2	1.53 ± 2.70	3.84 ± 8.72	74.51 ± 74.46	80.40 ± 72.92	
IO–	51.63 ± 6.94	52.56 ± 7.58	103.35 ± 34.41	109.48 ± 39.74	
I•	5.17 ± 5.32	5.32 ± 4.27	17.52 ± 18.48	24.28 ± 31.21	
O2•–	1.29 ± 0.81	1.83 ± 1.82	39.55 ± 31.28	47.45 ± 37.48	
OH–	4.74 ± 2.58	5.31 ± 3.11	30.78 ± 35.47	35.05 ± 38.29	
OH•	4.61 ± 1.32	5.15 ± 1.71	31.47 ± 25.88	35.93 ± 35.48	
a I indicates that
the initial concentrations are inside the bounds of the training set,
while E indicates that the initial concentrations
are outside the bounds.

Because the extrapolative ability of our RNN was poor compared
to its interpolative ability, we next examined combined interpolation/extrapolation
test sets in which the initial concentration of one species was within
the range of those in the training set, while that of the other was
outside of the range of the training set. As shown in Table 2, when the initial concentrations
of I2 were within the bounds of the training set, even
if those of I• were out of bounds, the sMAPE for
each species remained comparable to that when both species were within
the bounds. In the opposite situation, when I• was
within the bounds and I2 was out of bounds, the sMAPE for
each species was comparable to that when both species were out of
bounds, though slightly lower in all cases. Figure 4 shows the distribution of the average sMAPE
over all species for the four test sets. The interpolative test set
(denoted I2 (I) I• (I)) shows the tightest distribution with a mean sMAPE of
13.60 ± 1.66%. This is closely followed by the test set in which
the initial concentrations of I2 were within the bounds
and those of I• were out of bounds (I2 (I) I• (E)),
with a mean sMAPE of 14.40 ± 2.92%. The mean sMAPE for the contrasting
test set (I2 (E) I• (I)) greatly increased to 52.57 ± 30.95%, which is similar
to the mean sMAPE of the extrapolative test set (I2 (E) I• (E)) of 58.76 ±
35.57%. This examination indicates that the network is more sensitive
to the concentration of I2, likely owing to its order-of-magnitude
larger concentration compared with that of I•. The
sMAPEs of the I2 (E) I• (I) and I2 (E) I• (E) test sets range from 10 to 130%,
displaying the large variability in the predictions of simulations
in these two test sets.

Figure 4 Stacked bar plot of the sMAPE averaged over
all species for the
four distinct test sets. I indicates that the initial
concentrations of the preceding species are inside the bounds of the
training set, while E indicates that the initial
concentrations are outside the bounds. Each bar depicts the normalized
number of simulations producing the indicated sMAPE with a 5% deviation.

Effect of Training Data Size
The
RNNs discussed above
were trained on 1024 simulations. This number of simulations, while
common for deep learning applications in other domains (for comparison,
Makridakis et al. used a set of 1045 series for a comprehensive comparison
of deep learning versus statistical forecasting methods35), is often not feasible when, for instance,
one simulation corresponds to data from one experiment. Therefore,
we examined the effect of minimizing the number of simulations used
to train the network.

To create smaller training sets, we sampled
from the initial training set consisting of 1024 simulations. Because
the training of a network is highly dependent on the data structure,
we prepared 16 resamplings of the larger dataset for each dataset
of reduced size. We created five equal-sized bins based on the initial
concentrations of I2. We then sampled proportionately from
those bins to ensure that the training data were not skewed around
certain initial conditions. We created reduced datasets consisting
of 512, 256, 128, 96, 64, and 32 simulations. For comparative purposes,
the reported error is on the interpolative test set.

Figure 5 shows the
variation in the average sMAPE over all species as the training set
size decreases. The error bars represent the standard deviation in
the average sMAPE for each resampling. As expected, the sMAPE increases
as the training set size decreases; however, this decrease is not
linear. The average sMAPE from the network trained on 512 simulations
is 17%, compared to 14% for the network trained on 1024 simulations.
Decreasing the training set size fourfold to 128 simulations increases
the average sMAPE only by 4%. However, decreasing this set by a further
25% to 96 simulations increases the average sMAPE by another 5%, after
which the increase in the average sMAPE becomes increasingly larger.
In addition, the variation ranges between 1.8 and 3.0% until the training
set is reduced to 96 simulations, after which the variation increases
to above 5%. This indicates that as the number of examples in the
training set decreases, one must be more discerning of how the samples
are chosen to train the network.

Figure 5 Change in the sMAPE over all species as
the size of the training
set decreases. Sixteen resamplings were performed for each size training
set; the standard deviation in the sMAPE of the resamplings is shown
by the shaded error bars. The seed was kept constant over all trainings.
For a breakdown of how the error changes by species, see Figure S3
in the Supporting Information.

Network Variance
Because RNNs use their learned internal
representations, rather than exact templates from the training data,
to make predictions, the final network parameters depend on the initialization
conditions.16 This variability can lead
to different trained networks, the variability of which can be quantified
by training the network multiple times with different initial weights.
Therefore, we retrained our RNN using the largest training set (1024
simulations) with different initial weights, set according to the
NumPy and TensorFlow seeds. In each instance, the network converged
to a loss on the order of 10–6, but at different
rates (see Figure S1 for plots of the loss
during training). Notably, the same learning rate did not lead to
convergence for every initial set of weights. The default learning
rate for the Adam optimizer of 0.001 did not lead to convergence for
8 of the 16 seeds. In 6 cases, convergence was achieved by increasing
the initial learning rate to 0.005, indicating that the initial weights
were far from ideal. In the remaining 2 cases, convergence was achieved
by decreasing the initial learning rate to 0.0005, indicating that
these seeds provided good initial guesses for the network weights
and taking too large of an initial step moved the network out of the
ideal state. In all instances, an adaptive learning rate was used,
where after 20 epochs of no improvement in the loss of the validation
set, the learning rate was steadily decreased to a minimum of 0.0001.
The sMAPEs for each species for the 16 different initial weight configurations
were very similar (for a full listing of the sMAPEs for each initial
configuration, see Table S2), with averages
of 51.98 ± 4.20% for H+, 1.50 ± 1.08% for HOI,
0.36 ± 0.19% for I–, 4.72 ± 3.70% for
I2, 53.25 ± 3.53% for IO–, 4.98
± 2.24% for I•, 1.59 ± 0.60% for O2•–, 6.26 ± 3.52% for OH–, and 4.76 ± 1.45% for OH•.
Thus, if the network is converged to the lowest possible loss, the
predictions will be similar, though not identical.

Conclusions
We have demonstrated that neural networks can be used to predict
the chemical evolution of multiple species undergoing a series of
interconnected aqueous inorganic iodine reactions that can result
from an incident at a nuclear power plant. We showed that LSTM can
capture the dynamics of multiple species, over multiple time steps.
We also validated the usefulness of our network by considering the
inherent network variance and various training set sizes. We believe
that neural networks give rise to good surrogate models for chemistry
forecasting problems in which data are available but the underlying
mechanisms are not known. In practice, we envision that this technique
will be useful in a real-world scenario in which scientists have the
ability to capture data in real time via atmospheric sensors to examine
the environmental transfer of iodine after a nuclear reactor incident.
In such a situation, the identification of species may not be certain,
but their concentrations can be tracked over time. Neural networks
could then be used to perform accurate short-term forecasting of the
unknown species. However, working with real-world sensor data presents
its own challenges, such as anomalous readings, sensor failures, and
noisy data. Neural networks open a path to the data-driven prediction
of chemical evolutions that do not rely on explicit knowledge of the
chemical species or mechanisms.

Supporting Information Available
The Supporting Information is
available free of charge at https://pubs.acs.org/doi/10.1021/acsomega.9b04104.Subset of aqueous inorganic
iodine reactions used in
this work; plot of training loss of models with different seeds; and
sMAPE of each species as the size of the training set decreases with
different samplings of the full dataset (PDF)



Supplementary Material
ao9b04104_si_001.pdf

 The authors declare no
competing financial interest.

Acknowledgments
The authors thank Robert Rallo, Jan Strube, and
Mathew Thomas for insightful discussions. The research described in
this paper was conducted under the Laboratory Directed Research and
Development Program at Pacific Northwest National Laboratory (PNNL),
a multi-program national laboratory operated by Battelle for the U.S.
Department of Energy, Release no. PNNL-SA-148824.
==== Refs
References
Miyake Y. ; Matsuzaki H. ; Fujiwara T. ; Saito T. ; Yamagata T. ; Honda M. ; Muramatsu Y. 
Isotopic ratio of radioactive iodine
(129I/131I) released from Fukushima Daiichi NPP accident
. Geochem. J. 
2012 , 46 , 327 –333
. 10.2343/geochemj.2.0210 .
Lebel L. S. ; Dickson R. S. ; Glowa G. A. 
Radioiodine
in the atmosphere after
the Fukushima Dai-ichi nuclear accident
. J.
Environ. Radioact. 
2016 , 151 , 82 –93
. 10.1016/j.jenvrad.2015.06.001 .26440698 
Khazov Y. ; Mitropolsky I. ; Rodionov A. 
Nuclear Data Sheets for A = 131
. Nucl. Data Sheets 
2006 , 107 , 2715 –2930
. 10.1016/j.nds.2006.10.001 .
Takeyasu M. ; Nakano M. ; Fujita H. ; Nakada A. ; Watanabe H. ; Sumiya S. ; Furuta S. 
Results of
environmental radiation
monitoring at the nuclear fuel cycle engineering laboratories, JAEA,
following the Fukushima Daiichi nuclear power plant accident: Fukushima
NPP accident related
. J. Nucl. Sci. Technol. 
2012 , 49 , 281 –286
. 10.1080/00223131.2012.660014 .
Likhtarev I. A. ; Shandala N. K. ; Gulko G. M. ; Kairo I. A. ; Chepurny N. I. 
Ukrainian
thyroid doses after the Chernobyl accident
. Health Phys. 
1993 , 64 , 594 –599
. 10.1097/00004032-199306000-00003 .8491614 
Robbins J. ; Schneider A. B. 
Radioiodine-induced
thyroid cancer: studies in the
aftermath of the accident at Chernobyl
. Trends
Endocrinol. Metab. 
1998 , 9 , 87 –94
. 10.1016/s1043-2760(98)00024-1 .18406248 
Robbins J. ; Schneider A. B. 
Thyroid
cancer following exposure to radioactive iodine
. Rev. Endocr. Metab. Disord. 
2000 , 1 , 197 –203
. 10.1023/a:1010031115233 .11705004 
Zvonova I. ; Balonov M.  The Chernobyl Papers.
Doses to the Soviet Population and Early Health Effects Studies , Research Enterprises Inc. , 1993 ; Vol. 1 .
Unno N. ; Minakami H. ; Kubo T. ; Fujimori K. ; Ishiwata I. ; Terada H. ; Saito S. ; Yamaguchi I. ; Kunugita N. ; Nakai A. ; Yoshimura Y. 
Effect of
the Fukushima nuclear power plant accident on radioiodine (131I) content
in human breast milk
. J. Obstet. Gynaecol. Res. 
2012 , 38 , 772 –779
. 10.1111/j.1447-0756.2011.01810.x .22487003 
Yamashita S. ; Suzuki S. ; Suzuki S. ; Shimura H. ; Saenko V. 
Lessons from
Fukushima: latest findings of thyroid cancer after the Fukushima nuclear
power plant accident
. Thyroid 
2018 , 28 , 11 –22
. 10.1089/thy.2017.0283 .28954584 
Vikis A. C. ; MacFarlane R. 
Reaction of iodine with ozone in the gas phase
. J. Phys. Chem. 
1985 , 89 , 812 –815
. 10.1021/j100251a019 .
Wren J. C. ; Ball J. M. 
LIRIC 3.2 an updated
model for iodine behaviour in
the presence of organic impurities
. Radiat.
Phys. Chem. 
2001 , 60 , 577 –596
. 10.1016/s0969-806x(00)00385-6 .
Guentay S. ; Cripps R. C. ; Jäckel B. ; Bruchertseifer H. 
Iodine behaviour
during a severe accident in a nuclear power plant
. CHIMIA 
2005 , 59 , 957 –965
. 10.2533/000942905777675453 .
Bosland L. ; Funke F. ; Girault N. ; Langrock G. 
PARIS project: Radiolytic
oxidation of molecular iodine in containment during a nuclear reactor
severe accident: Part 1. Formation and destruction of air radiolysis
productsExperimental results and modelling
. Nucl. Eng. Des. 
2008 , 238 , 3542 –3550
. 10.1016/j.nucengdes.2008.06.023 .
Bosland L. ; Funke F. ; Langrock G. ; Girault N. 
PARIS project: Radiolytic
oxidation of molecular iodine in containment during a nuclear reactor
severe accident: Part 2. Formation and destruction of iodine oxides
compounds under irradiation—Experimental results modelling
. Nucl. Eng. Des. 
2011 , 241 , 4026 –4044
. 10.1016/j.nucengdes.2011.06.015 .
Graves A.  Generating sequences
with recurrent neural networks
. 2013 ,
arXiv:1308.0850. arXiv preprint.
Wu Y. ; Schuster M. ; Chen Z. ; Le Q. V. ; Norouzi M. ; Macherey W. ; Krikun M. ; Cao Y. ; Gao Q. ; Macherey K.  Google’s neural machine translation system:
Bridging the gap between human and machine translation
. 2016 , arXiv:1609.08144. arXiv preprint.
Vinyals O. ; Toshev A. ; Bengio S. ; Erhan D.  Show and tell: A neural image
caption generator
. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition . 2015 ; pp 3156 –3164
.
Hochreiter S. ; Schmidhuber J. 
Long short-term memory
. Neural
Comput. 
1997 , 9 , 1735 –1780
. 10.1162/neco.1997.9.8.1735 .9377276 
Hochreiter S. ; Bengio Y. ; Frasconi P. ; Schmidhuber J.  Gradient
flow in recurrent nets: the difficulty of learning long-term dependencies
. A Field Guide to Dynamical Recurrent Networks ; IEEE , 2001 .
Bini R. ; Chiappe C. ; Duce C. ; Micheli A. ; Solaro R. ; Starita A. ; Tiné M. R. 
Ionic liquids: prediction of their
melting points by a recursive neural network model
. Green Chem. 
2008 , 10 , 306 –309
. 10.1039/b708123e .
Lusci A. ; Pollastri G. ; Baldi P. 
Deep architectures and deep learning
in chemoinformatics: the prediction of aqueous solubility for drug-like
molecules
. J. Chem. Inf. Model. 
2013 , 53 , 1563 –1575
. 10.1021/ci400187y .23795551 
Liu B. ; Ramsundar B. ; Kawthekar P. ; Shi J. ; Gomes J. ; Luu Nguyen Q. ; Ho S. ; Sloane J. ; Wender P. ; Pande V. 
Retrosynthetic reaction prediction using neural sequence-to-sequence
models
. ACS Cent. Sci. 
2017 , 3 , 1103 –1113
. 10.1021/acscentsci.7b00303 .29104927 
Fooshee D. ; Mood A. ; Gutman E. ; Tavakoli M. ; Urban G. ; Liu F. ; Huynh N. ; Van Vranken D. ; Baldi P. 
Deep learning for chemical
reaction prediction
. Mol. Syst. Des. Eng. 
2018 , 3 , 442 –452
. 10.1039/c7me00107j .
Gupta A. ; Müller A. T. ; Huisman B. J. H. ; Fuchs J. A. ; Schneider P. ; Schneider G. 
Generative recurrent networks for de novo drug design
. Mol. Inf. 
2018 , 37 , 1700111 10.1002/minf.201700111 .
Müller A. T. ; Hiss J. A. ; Schneider G. 
Recurrent
neural network model for
constructive peptide design
. J. Chem. Inf. Model. 
2018 , 58 , 472 –479
. 10.1021/acs.jcim.7b00414 .29355319 
Sanchez-Lengeling B. ; Aspuru-Guzik A. 
Inverse molecular design using machine
learning: Generative
models for matter engineering
. Science 
2018 , 361 , 360 –365
. 10.1126/science.aat2663 .30049875 
Poinssot C. ; Ferry C. ; Kelm M. ; Cavedon J. ; Corbel C. ; Jegou C. ; Lovera P. ; Miserque F. ; Poulesquen A. ; Grambow B.  Spent Fuel Stability under Repository Conditions—Final
Report of the European Project , 2005 .
Pastina B. ; LaVerne J. A. 
Effect of molecular hydrogen on hydrogen
peroxide in
water radiolysis
. J. Phys. Chem. A 
2001 , 105 , 9316 –9322
. 10.1021/jp012245j .
Dahlgren B. 
ChemPy: A
package useful for chemistry written in Python
. J. Open Source Software 
2018 , 3 , 565 10.21105/joss.00565 .
Hirsch M. W. ; Smale S. ; Devaney R. L.  Differential
Equations,
Dynamical Systems, and an Introduction to Chaos ; Academic press , 2012 .
Glorot X. ; Bordes A. ; Bengio Y.  Deep sparse rectifier
neural networks
. Proceedings of the Fourteenth
International Conference on Artificial Intelligence and Statistics , 2011 ; pp 315 –323
.
Kingma D. P. ; Ba J.  Adam: A method for stochastic
optimization
. 2014 , arXiv preprint arXiv:1412.6980.
Makridakis S. 
Accuracy measures:
theoretical and practical concerns
. Int. J.
Forecast. 
1993 , 9 , 527 –529
. 10.1016/0169-2070(93)90079-3 .
Makridakis S. ; Spiliotis E. ; Assimakopoulos V. 
Statistical and Machine Learning
forecasting methods: Concerns and ways forward
. PLoS One 
2018 , 13 , e019488910.1371/journal.pone.0194889 .29584784

